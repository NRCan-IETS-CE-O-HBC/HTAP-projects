{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "anticipated-mason",
   "metadata": {},
   "source": [
    "# Creating housing archetypes using K-means method\n",
    "### Q4 2020-21\n",
    "\n",
    "## Introduction\n",
    "This workbook will ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from kneed import KneeLocator \n",
    "from sklearn.datasets import make_blobs \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from itertools import permutations\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-definition",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "The data represents a subset of ERS records used for initial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-blond",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ers_sample_records = pd.read_csv(r\"C:\\Users\\owner\\Documents\\NRCan\\code\\practice\\InitialHousingData.csv\",)\n",
    "ers_sample_records \n",
    "\n",
    "#original amount of obsevations/rows \n",
    "og_obs = ers_sample_records.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-world",
   "metadata": {},
   "source": [
    "## Database Preperation and Varibable Selection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-drink",
   "metadata": {},
   "source": [
    "### Select variables for clustering\n",
    "Placeholder for this process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Air50P selected for 1D clustering test\n",
    "cl_variables = ['Air50P']\n",
    "test_data = ers_sample_records[cl_variables]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-primary",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-stock",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove rows with blank values\n",
    "test_data_cleaned = test_data.dropna()\n",
    "#display how many rows removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-capital",
   "metadata": {},
   "source": [
    "###  inconsistent data removal \n",
    "each variable has own range that is should be within. Airt tightness >0 ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-chicago",
   "metadata": {},
   "source": [
    "### Ensure all values are numerical\n",
    "and positive? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is catagorical information use LabelEncoder,onehotenoder from scikitlearn or ... creat if str then, if not then float\n",
    "\n",
    "# ensure all vales are floats \n",
    "test_data_cleaned.astype('float') #float64?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-stuart",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-audience",
   "metadata": {},
   "source": [
    "Visualize with box plot to confirm outlier removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-barcelona",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#box plot before outlier removal\n",
    "test_data_cleaned.boxplot() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-murray",
   "metadata": {},
   "source": [
    "#### Local outlier factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-softball",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define the model\n",
    "\n",
    "lof = LocalOutlierFactor()\n",
    "lof_pred = lof.fit_predict(test_data_cleaned) \n",
    "\n",
    "#We'll extract the negative outputs as the outliers.\n",
    "mask = lof_pred != -1\n",
    "\n",
    "#remove rows with outliers \n",
    "lof_data = test_data_cleaned[mask] #i dont think this works for sets with more than 1 varibale\n",
    "print([lof_data])\n",
    "\n",
    "#print amount of points deleated\n",
    "\n",
    "outliers_rem = og_obs - lof_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )\n",
    "\n",
    "#plot without after removal\n",
    "\n",
    "lof_data.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-mechanism",
   "metadata": {},
   "source": [
    "#### Z-score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find absolute value of z-score for each observation\n",
    "z = np.abs(stats.zscore(test_data_cleaned))\n",
    "\n",
    "#only keep rows in dataframe with all z-scores less than absolute value of 3 \n",
    "z_data = test_data_cleaned[(z<3).all(axis=1)]\n",
    "\n",
    "#find how many rows are left in the dataframe \n",
    "z_data\n",
    "\n",
    "outliers_rem = og_obs - z_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot \n",
    "z_data.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-nicaragua",
   "metadata": {},
   "source": [
    "#### IRQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find Q1, Q3, and interquartile range for each column\n",
    "Q1 =test_data_cleaned.quantile(q=.25)\n",
    "Q3 = test_data_cleaned.quantile(q=.75)\n",
    "IQR =test_data_cleaned.apply(stats.iqr)\n",
    "\n",
    "#only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3\n",
    "iqr_data = test_data_cleaned[~((test_data_cleaned < (Q1-1.5*IQR)) | (test_data_cleaned > (Q3+1.5*IQR))).any(axis=1)]\n",
    "\n",
    "#find how many rows are left in the dataframe \n",
    "\n",
    "outliers_rem = og_obs - iqr_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot \n",
    "iqr_data.boxplot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-concert",
   "metadata": {},
   "source": [
    " Mahalanobis distance - calc when multi varibale later\n",
    " https://www.statology.org/mahalanobis-distance-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-killer",
   "metadata": {},
   "source": [
    "### clusterss stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-basketball",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create sets of pre-processed data ready for clustering \n",
    "scaled_data_sets = [iqr_data,lof_data, z_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of scalers\n",
    "standard = StandardScaler()\n",
    "minimax = MinMaxScaler()\n",
    "scalers = [standard, minimax]\n",
    "#list of initalizers \n",
    "r = 'random'\n",
    "plus = 'k-means++' \n",
    "initalizer = [r, plus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-gospel",
   "metadata": {},
   "source": [
    "#need to figure out how to use paramgrid to do this\n",
    "#permutatins\n",
    "perm_amount = len(scalers)*len(initalizer)*len(scaled_data_sets)\n",
    "\n",
    "i=0 # data set\n",
    "j=0 #scaler\n",
    "l=0 #init\n",
    "m=0 #rounds \n",
    "while i != len(scaled_data_sets):\n",
    "     #first set of data to scale\n",
    "    scaled_features = scalers[j].fit_transform(scaled_data_sets[i]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elblw method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":initalizer[l],  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=initalizer[l],\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    print(kmeans.inertia_) # can use other validations too \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets being transfromed with standard scaler and random init\n",
    "standard_rand=[]\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[0].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elblw method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"random\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    standard_rand.append(kmeans.inertia_) # can use other validations too \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets being transfromed with mini scaler and random init\n",
    "mini_rand=[]\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[1].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elblw method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"random\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    mini_rand.append(kmeans.inertia_) # can use other validations too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets being transfromed with mini scaler and k++ init\n",
    "mini_plus=[]\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[1].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elblw method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    mini_plus.append(kmeans.inertia_) # can use other validations too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets being transfromed with standard scaler and k++ init\n",
    "standard_plus=[]\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[0].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elblw method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    standard_plus.append(kmeans.inertia_) # can use other validations too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-input",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "results = pd.DataFrame({ 'st rand':standard_rand, 'st plus':standard_plus,'min rand':mini_rand, 'min plus': mini_plus}, index= ['IRQ','LOF','Z-score'])\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose best scheme based on interia \n",
    "results.iloc[1,0] #LOF, Standard, Plus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-luther",
   "metadata": {},
   "source": [
    "### Clusteting with determined parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-festival",
   "metadata": {},
   "source": [
    "Take parameters from above and re-cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-psychiatry",
   "metadata": {},
   "source": [
    "Cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-brisbane",
   "metadata": {},
   "source": [
    "Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-investing",
   "metadata": {},
   "source": [
    "#### Resutling Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-nursing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#cl_labels=kmeans.labels_ #create variale for lables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-simpson",
   "metadata": {},
   "source": [
    " add cluster labes as a column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-grave",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cl_data.plot.scatter(x='Air50P',y='clusterNo')#plot clusters. when multi-dim need to figure out how to colour points by cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cl_data.value_counts('clusterNo') #number of houses in each cluster "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-dividend",
   "metadata": {},
   "source": [
    "### Silhouette Coefficicent \n",
    "another clustering amount determination method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_coefficients =[]\n",
    "for k in range(2,11):\n",
    "    kmeans=KMeans(n_clusters=k,**kmeans_kwargs)\n",
    "    kmeans.fit(scaled_features)\n",
    "    score = silhouette_score(scaled_features, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(2, 11), silhouette_coefficients)\n",
    "plt.xticks(range(2, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
