{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bigger-olive",
   "metadata": {},
   "source": [
    "# Creating housing archetypes using K-means method\n",
    "### Q4 2020-21\n",
    "\n",
    "## Introduction\n",
    "This workbook will ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from kneed import KneeLocator \n",
    "from sklearn.datasets import make_blobs \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from itertools import permutations\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-definition",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "The data represents a subset of ERS records used for initial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-blond",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ers_sample_records = pd.read_csv(r\"C:\\Users\\owner\\Documents\\NRCan\\code\\practice\\InitialHousingData.csv\",)\n",
    "ers_sample_records \n",
    "\n",
    "#original amount of obsevations/rows \n",
    "og_obs = ers_sample_records.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-workplace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ers_sample_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "ers_sample_records.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-world",
   "metadata": {},
   "source": [
    "## Database Preperation and Varibable Selection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-drink",
   "metadata": {},
   "source": [
    "### Select variables for clustering\n",
    "Placeholder for this process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Air50P selected for 1D clustering test\n",
    "cl_variables = ['Main']\n",
    "test_data = ers_sample_records[cl_variables]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-primary",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-stock",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove rows with blank values\n",
    "test_data_cleaned = test_data.dropna()\n",
    "#display how many rows removed\n",
    "test_data_cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-capital",
   "metadata": {},
   "source": [
    "###  Inconsistent data removal \n",
    "Dependant on the variable. Example: floor area could not be negavtive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-chicago",
   "metadata": {},
   "source": [
    "### Ensure all values are numerical\n",
    "K-means can only handle numerical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode catagorical variables \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-stuart",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-audience",
   "metadata": {},
   "source": [
    "K-means is based off finding the mean of clusters and since means are senstive to outliers so is k-means. If outliers are not delt with the they can have a large infulence in the clustering porcess that could result in poor partiotns that are not representaive of the data. Because of this outliers need to be removed properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-seeking",
   "metadata": {},
   "source": [
    "Three different outlier removal techniques are tested: local outlier factor (LOF), z-score, and inter quartile range (IQR). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-corner",
   "metadata": {},
   "source": [
    "The data will be interperated before and after outlier removal though box plots( and scatter?) and statisical measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-barcelona",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#box plot before outlier removal\n",
    "test_data_cleaned.boxplot() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-murray",
   "metadata": {},
   "source": [
    "#### Local outlier factor\n",
    "Local outlier factor (LOF) values identify an outlier based on the local neighborhood. It gives better results than the global approach to find outliers. A point will be considered as an outlier if it is at a small distance to the extremely dense cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-softball",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define the model\n",
    "\n",
    "lof = LocalOutlierFactor()\n",
    "lof_pred = lof.fit_predict(test_data_cleaned) \n",
    "\n",
    "#extract the negative outputs as the outliers.\n",
    "mask = lof_pred != -1\n",
    "\n",
    "#remove rows with outliers \n",
    "lof_data = test_data_cleaned[mask] #i dont think this works for sets with more than 1 varibale\n",
    "print([lof_data])\n",
    "\n",
    "#print amount of points deleated\n",
    "\n",
    "outliers_rem = og_obs - lof_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )\n",
    "\n",
    "#plot without after removal\n",
    "\n",
    "lof_data.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-mechanism",
   "metadata": {},
   "source": [
    "#### Z-score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find absolute value of z-score for each observation\n",
    "z = np.abs(stats.zscore(test_data_cleaned))\n",
    "\n",
    "#only keep rows in dataframe with all z-scores less than absolute value of 3 \n",
    "z_data = test_data_cleaned[(z<3).all(axis=1)]\n",
    "\n",
    "#print amount of outliers removed\n",
    "z_data\n",
    "\n",
    "outliers_rem = og_obs - z_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotafter outlier removal\n",
    "z_data.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-nicaragua",
   "metadata": {},
   "source": [
    "#### IQR\n",
    "The IQR can be used to identify outliers by defining limits on the sample values that are a factor k of the IQR below the 25th percentile or above the 75th percentile. The common value for the factor k is the value 1.5 (used below). A factor k of 3 or more can be used to identify values that are extreme outliers or “far outs” when described in the context of box and whisker plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find Q1, Q3, and interquartile range for each column\n",
    "Q1 =test_data_cleaned.quantile(q=.25)\n",
    "Q3 = test_data_cleaned.quantile(q=.75)\n",
    "IQR =test_data_cleaned.apply(stats.iqr)\n",
    "\n",
    "#only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3\n",
    "iqr_data = test_data_cleaned[~((test_data_cleaned < (Q1-1.5*IQR)) | (test_data_cleaned > (Q3+1.5*IQR))).any(axis=1)]\n",
    "\n",
    "#print amount of outliers removed\n",
    "\n",
    "outliers_rem = og_obs - iqr_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-breakdown",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot of IQR outlier removal\n",
    "iqr_data.boxplot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-concert",
   "metadata": {},
   "source": [
    " Mahalanobis distance - calc when multi variabale later\n",
    " https://www.statology.org/mahalanobis-distance-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-killer",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "Find the best mix of parameters for clustering. Parameters include oulier removal, scaling, initalization, and k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import validation metrics \n",
    "#could also use package validclust\n",
    "from sklearn.metrics import (silhouette_score, calinski_harabasz_score, davies_bouldin_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-basketball",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create sets of pre-processed data ready for clustering \n",
    "scaled_data_sets = [iqr_data,lof_data, z_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of scalers\n",
    "standard = StandardScaler()\n",
    "minimax = MinMaxScaler()\n",
    "scalers = [standard, minimax]\n",
    "#list of initalizers \n",
    "r = 'random'\n",
    "plus = 'k-means++' \n",
    "initalizer = [r, plus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets being transfromed with standard scaler and random init\n",
    "standard_rand_in=[]\n",
    "standard_rand_cal=[]\n",
    "standard_rand_dav=[]\n",
    "standard_rand_sil=[]\n",
    "\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[0].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elblw method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"random\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    standard_rand_in.append(kmeans.inertia_) # can use other validations too \n",
    "    \n",
    "    pred_labels = kmeans.labels_\n",
    "    \n",
    "    sil = silhouette_score(scaled_data_sets[x], pred_labels)\n",
    "    cal_score = calinski_harabasz_score(scaled_data_sets[x], pred_labels)\n",
    "    dav_score = davies_bouldin_score(scaled_data_sets[x], pred_labels)\n",
    "    \n",
    "    standard_rand_cal.append(cal_score)\n",
    "    standard_rand_dav.append(dav_score)\n",
    "    standard_rand_sil.append(sil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets being transfromed with mini scaler and random init\n",
    "mini_rand_in=[]\n",
    "mini_rand_cal=[]\n",
    "mini_rand_sil=[]\n",
    "mini_rand_dav=[]\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[1].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elblw method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"random\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "   \n",
    "    #validation metrics \n",
    "    \n",
    "    mini_rand_in.append(kmeans.inertia_) # can use other validations too \n",
    "   \n",
    "    pred_labels = kmeans.labels_\n",
    "    \n",
    "    sil = silhouette_score(scaled_data_sets[x], pred_labels)\n",
    "    cal_score = calinski_harabasz_score(scaled_data_sets[x], pred_labels)\n",
    "    dav_score = davies_bouldin_score(scaled_data_sets[x], pred_labels)\n",
    "\n",
    "    mini_rand_cal.append(cal_score)\n",
    "    mini_rand_sil.append(sil)\n",
    "    mini_rand_dav.append(dav_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets being transfromed with mini scaler and k++ init\n",
    "\n",
    "mini_plus_in=[]\n",
    "mini_plus_cal=[]\n",
    "mini_plus_sil=[]\n",
    "mini_plus_dav=[]\n",
    "\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[1].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elblw method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    mini_plus_in.append(kmeans.inertia_) # can use other validations too \n",
    "    \n",
    "    pred_labels = kmeans.labels_\n",
    "    \n",
    "    sil = silhouette_score(scaled_data_sets[x], pred_labels)\n",
    "    cal_score = calinski_harabasz_score(scaled_data_sets[x], pred_labels)\n",
    "    dav_score = davies_bouldin_score(scaled_data_sets[x], pred_labels)\n",
    "  \n",
    "    mini_plus_cal.append(cal_score)\n",
    "    mini_plus_sil.append(sil)\n",
    "    mini_plus_dav.append(dav_score)\n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets being transfromed with standard scaler and k++ init\n",
    "\n",
    "standard_plus_in=[]\n",
    "standard_plus_cal=[]\n",
    "standard_plus_dav=[]\n",
    "standard_plus_sil=[]\n",
    "\n",
    "\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[0].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elbow method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    standard_plus_in.append(kmeans.inertia_) # can use other validations too \n",
    "     \n",
    "    pred_labels = kmeans.labels_\n",
    "    \n",
    "    sil = silhouette_score(scaled_data_sets[x], pred_labels)\n",
    "    cal_score = calinski_harabasz_score(scaled_data_sets[x], pred_labels)\n",
    "    dav_score = davies_bouldin_score(scaled_data_sets[x], pred_labels)\n",
    "    \n",
    "    standard_plus_cal.append(cal_score)\n",
    "    standard_plus_dav.append(dav_score)\n",
    "    standard_plus_sil.append(sil)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-input",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#intertia results\n",
    "results_inertia = pd.DataFrame({ 'st rand':standard_rand_in, 'st plus':standard_plus_in,'min rand':mini_rand_in, 'min plus': mini_plus_in}, index= ['IRQ','LOF','Z-score'])\n",
    "results_inertia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DB results(Values closer to zero indicate a better partition.)\n",
    "results = pd.DataFrame({ 'st rand':standard_rand_dav, 'st plus':standard_plus_dav,'min rand':mini_rand_dav, 'min plus': mini_plus_dav}, index= ['IRQ','LOF','Z-score'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sil results (want higher=highly dense clustering)\n",
    "results = pd.DataFrame({ 'st rand':standard_rand_sil, 'st plus':standard_plus_sil,'min rand':mini_rand_sil, 'min plus': mini_plus_sil}, index= ['IRQ','LOF','Z-score'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cal results(want higher)\n",
    "results = pd.DataFrame({ 'st rand':standard_rand_cal, 'st plus':standard_plus_cal,'min rand':mini_rand_cal, 'min plus': mini_plus_cal}, index= ['IRQ','LOF','Z-score'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-guidance",
   "metadata": {},
   "source": [
    "## Result visualization \n",
    "Visulaizing the results of the nest method determined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-experiment",
   "metadata": {},
   "source": [
    "LOF outlier ommision, MinMax scaling, elbow method k determination, k-means++ initalization. Independant od Clustering performed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-monaco",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data\n",
    "data = lof_data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-contribution",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merging original data with cleaned data for plotting realtionships later\n",
    "lof_data_full = pd.merge(data, ers_sample_records, right_index=True, left_index =True) \n",
    "lof_data_full\n",
    "lof_data_full = lof_data_full.drop(columns=['Rating_x'])\n",
    "#re-setting index for mergeing with cluster lables later\n",
    "data_all = lof_data_full\n",
    "data_all = data_all.reset_index(drop=True)\n",
    "data_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot before clustering to look for intutive clusters \n",
    "sns.pairplot(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling with MinMax scaler\n",
    "minmax = MinMaxScaler()\n",
    "scaled_features = minmax.fit_transform(data)\n",
    "#scaled feature into a data frame \n",
    "scaled_features = pd.DataFrame(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-tucson",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#compare Stats for scaled and unsclaed data\n",
    "#show scaled stats\n",
    "scaled_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show unscaled stats\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-accent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot histogram of scaled feaures \n",
    "scaled_features.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-junction",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# before scaling histogram\n",
    "data.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering\n",
    "\n",
    "kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\", \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "\n",
    "sse=[] #determine SSE for 1 to 11 clusters    \n",
    "for k in range(1,11): \n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "        \n",
    "#plot elbow method\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1,11), sse)\n",
    "plt.xticks(range(1,11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()\n",
    "        \n",
    "k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "clamount=k1.elbow\n",
    "    \n",
    "#cluster with k determined above \n",
    "    \n",
    "kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-belle",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a dataframe including all variabels and cluster labels \n",
    "kmeans.fit(scaled_features)\n",
    "labels = pd.DataFrame(kmeans.labels_,columns=['cluster label'])\n",
    "\n",
    "cluster1=pd.concat([labels,data_all], axis = 1) \n",
    "\n",
    "cluster1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-venture",
   "metadata": {},
   "source": [
    "Looking at clusters individually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-window",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cluster 0 dataframe\n",
    "cluster_0 = cluster1.loc[cluster1['cluster label'] == 0, ['MainWallIns_y']]\n",
    "cluster_0 = cluster_0.rename(columns={'MainWallIns_y': 'Cluster 0'})\n",
    "# can view boxplot and stats separate but easier to compare on same table see below \n",
    "#cluster_0.boxplot()\n",
    "#cluster_0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-pledge",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#cluster 1 dataframe\n",
    "cluster_1 = cluster1.loc[cluster1['cluster label'] == 1, ['MainWallIns_y']]\n",
    "cluster_1=cluster_1.rename(columns={'MainWallIns_y': 'Cluster 1'})\n",
    "#cluster_1.boxplot()\n",
    "#cluster_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-surgery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cluster 2 dataframe\n",
    "cluster_2 = cluster1.loc[cluster1['cluster label'] == 2, ['MainWallIns_y']]\n",
    "#cluster_2.boxplot()\n",
    "#cluster_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-festival",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cancatinate blox plot for each cluster to compare variation they capture for the certain vairbale \n",
    "#concatinate into a data frame \n",
    "all_clust = pd.concat([cluster_0,cluster_1, cluster_2], ignore_index=True, axis=1)\n",
    "#plot using pandas \n",
    "all_clust.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-respondent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#comparison of all cluster stats \n",
    "all_clust.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluser Centroids \n",
    "\n",
    "centroids = minmax.inverse_transform(kmeans.cluster_centers_) # transform scaled cenroids back\n",
    "\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-approach",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Scatter plot of custers not scaled \n",
    "sns.scatterplot('MainWallIns_y', 'YearBuilt', data=cluster1, hue= 'cluster label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot of scaled values \n",
    "\n",
    "# create a dataframe including all variabels and cluster labels \n",
    "kmeans.fit(scaled_features)\n",
    "labels = pd.DataFrame(kmeans.labels_,columns=['cluster label'])\n",
    "\n",
    "data_all_scaled = pd.DataFrame(minmax.fit_transform(data_all), columns = [ 'Air50P', 'Rating', 'YearBuilt', 'FloorArea',\n",
    "       'MainWallIns_y'])\n",
    "\n",
    "clusters_scaled = pd.concat([labels, data_all_scaled], axis = 1) \n",
    "sns.pairplot(clusters_scaled, hue = 'cluster label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-logic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot all varibles (not scaled) against eachother to find patterns in clusters  \n",
    "sns.pairplot(cluster1, hue='cluster label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-cabinet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#silhouette coefficent visualizer\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "\n",
    "visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n",
    "\n",
    "visualizer.fit(scaled_features)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "#In SilhouetteVisualizer plots, clusters with higher scores have wider silhouettes, but clusters \n",
    "#that are less cohesive will fall short of the average score across all clusters, which is plotted as a\n",
    "#vertical dotted red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inter cluster distance maps\n",
    "from yellowbrick.cluster import InterclusterDistance\n",
    "\n",
    "visualizer = InterclusterDistance(kmeans)\n",
    "\n",
    "visualizer.fit(scaled_features)    # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "#the closer to centers are in the visualization, the closer they are in the original feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-cross",
   "metadata": {},
   "source": [
    "## 2D clustering \n",
    "This area is completely separate from above 1D clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-links",
   "metadata": {},
   "source": [
    "### Data preping \n",
    "Fist the data is retrieved and preped. Missing data is removed because k-means does not accept missing values and outliers are also removed because K-means is sensitive to outliers (a mean is easily influenced by extreme value). The preping will help acheive more robust clusting results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retreive data\n",
    "ers_sample_records = pd.read_csv(r\"C:\\Users\\owner\\Documents\\NRCan\\code\\practice\\InitialHousingData.csv\",)\n",
    "ers_sample_records \n",
    "\n",
    "#original amount of obsevations/rows \n",
    "og_obs = ers_sample_records.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-draft",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#retrive data from above\n",
    "ers_sample_records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-employer",
   "metadata": {},
   "source": [
    "Two variables are selected fo perform clustering on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-judges",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#select variables for 2D clustering \n",
    "cl_variables = ['YearBuilt', 'Rating'] # for automation of code (hasn't been done yet)\n",
    "twoD_data = ers_sample_records[cl_variables] # a varible towD_data is created to hold clustering varibles specifcally for 2D analysis \n",
    "twoD_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-unemployment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#review raw data stats \n",
    "twoD_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-valve",
   "metadata": {},
   "source": [
    "Cleaning data. Dropna from pandas is used to remove rows were at least on element is na and local outlier factor is applied for outlier detection. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-battery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Drop the rows where at least one element is NA.\n",
    "twoD_data_clean = twoD_data.dropna()\n",
    "removed = len(twoD_data)-len(twoD_data_clean)\n",
    "#print amount of rows removed \n",
    "print('rows of data dropped:%d' %removed )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-cursor",
   "metadata": {},
   "source": [
    "Local outlier factor (LOF) values identify an outlier based on the local neighborhood. It gives better results than the global approach to find outliers. A point will be considered as an outlier if it is at a small distance to the extremely dense cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot varation with box plots before outlier removal to visualize outliers \n",
    "twoD_data.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-springfield",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define the model\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20)\n",
    "lof_pred = lof.fit_predict(twoD_data_clean) \n",
    "\n",
    "#extract the negative outputs as the outliers.\n",
    "mask = lof_pred != -1\n",
    "\n",
    "#remove rows with outliers \n",
    "lof_data = twoD_data_clean[mask] #i dont think this works for sets with more than 1 varibale\n",
    "\n",
    "#print amount of points deleated\n",
    "\n",
    "outliers_rem = og_obs - lof_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )\n",
    "\n",
    "#plot without after removal to visualize outlier removal\n",
    "\n",
    "lof_data.boxplot() \n",
    "\n",
    "#should scale and plot as they have very different scales "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-advance",
   "metadata": {},
   "source": [
    "Also can visualize outlier remvial though a scatter plot. The red values are the outliers that have been removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of removed outliers for plotting \n",
    "from numpy import where\n",
    "lofs_index = where(lof_pred==-1) \n",
    "#Filter_df  = twoD_data_clean[twoD_data_clean.index.isin(lofs_index)]\n",
    "outliers =twoD_data_clean.loc[lofs_index] #datarfame of outliers \n",
    "#could write a check here: len of oultleris == outliers rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(twoD_data_clean['Air50P'], twoD_data_clean['MainWallIns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-framing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot removed outliers in red\n",
    "plt.scatter(twoD_data_clean['Air50P'], twoD_data_clean['MainWallIns'])\n",
    "plt.scatter(outliers['Air50P'],outliers['MainWallIns'], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-portrait",
   "metadata": {},
   "source": [
    "Compare cleaned data stats to raw data to confirm the data is still meaningful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-dublin",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#stats after outlier removal \n",
    "lof_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "twoD_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-passion",
   "metadata": {},
   "source": [
    "### Scaling \n",
    "k-means is distance based so for it to consider all attributes as equal and produce unbiased resutls, they must all have the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-wagon",
   "metadata": {},
   "source": [
    "Min-max scaling rescales the data into a given range, in this case 0-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()\n",
    "scaled_features = minmax.fit_transform(lof_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-nutrition",
   "metadata": {},
   "source": [
    "### Clustering \n",
    "In this step the preped data is clustered with the k-means algorithm. K-means++ is used for inialization and the amount of clusters is determined with internal validity measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-surrey",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-saint",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "# Instantiate the clustering model and visualizer\n",
    "# within cluster SSe (aka distortion)\n",
    "#Distortion is the average of the euclidean squared distance from the centroid of the respective clusters. \n",
    "#Inertia is the sum of squared distances of samples to their closest cluster centre\n",
    "model = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "\n",
    "#plot elbow method with distortion score\n",
    "\n",
    "visualizer = KElbowVisualizer(model, k=(1,10))\n",
    "\n",
    "visualizer.fit(lof_data)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "\n",
    "#plot elbow method Calinski Harabasz \n",
    "\n",
    "visualizer = KElbowVisualizer(model, k=(2,10),metric='calinski_harabasz')\n",
    "\n",
    "visualizer.fit(lof_data)        # Fit the data to the visualizer\n",
    "visualizer.show()    \n",
    "\n",
    "#plot elbow method with silhouette score \n",
    "\n",
    "visualizer = KElbowVisualizer(model, k=(2,10),metric='silhouette')\n",
    "\n",
    "visualizer.fit(lof_data)        # Fit the data to the visualizer\n",
    "visualizer.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-western",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plot elbow method with inertia/WSS/SSE\n",
    "\n",
    "sse=[] #determine SSE for 1 to 11 clusters, SSE = WSS\n",
    "kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",\n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,\n",
    "}\n",
    "\n",
    "for k in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(scaled_features)\n",
    "    sse.append(kmeans.inertia_)\n",
    " \n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1,11), sse)\n",
    "plt.xticks(range(1,11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the elbow/k\n",
    "k1 = KneeLocator(\n",
    "    range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "k1.elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-combination",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "DB_score = []\n",
    "\n",
    "kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",\n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,\n",
    "}\n",
    "\n",
    "for k in range(2,11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(scaled_features)\n",
    "    pred_labels = kmeans.labels_\n",
    "    dav_score = davies_bouldin_score(scaled_features, pred_labels)\n",
    "    DB_score.append(dav_score)\n",
    "   \n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(2,11), DB_score)\n",
    "plt.xticks(range(2,11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"DB\")\n",
    "plt.show()\n",
    "\n",
    "#find min BD = optimum K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-importance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#amount of clusters for DB\n",
    "min_index = DB_score.index(min(DB_score))\n",
    "DB_k =range(2,11)\n",
    "DB_k = DB_k[min_index]\n",
    "DB_k "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-panama",
   "metadata": {},
   "source": [
    "After the optimal k vlaue the clustering algortihm is ran once more and resulting clusters can be analyzed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-synthesis",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#final k-means arguments \n",
    "kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters=3, #change k value here\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "kmeans.fit(scaled_features)\n",
    "labels = pd.DataFrame(kmeans.labels_,columns=['label']) #creating a dataframe to house cluster labels for each house\n",
    "centroids = kmeans.cluster_centers_ #creating varible to hold centroid values \n",
    "centroids_real = minmax.inverse_transform(centroids) #also mean of cluster\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_data_new_index = lof_data.reset_index(drop=True) #new index to allow concatination to line up properly \n",
    "\n",
    "clusters = pd.concat([labels,lof_data_new_index], axis = 1) #can concatanate along columns but enssure that indicies are the same\n",
    "#using groupby to compare clustering results \n",
    "label_group = clusters.groupby(['label']) #make varible for lable groups\n",
    "\n",
    "label_group.get_group(1) #retrieve data frame of group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using group by to get median and means(cenroids) of the resulting clusters \n",
    "label_group['FloorArea'].agg(['mean','median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#geting mean and median for just one cluster \n",
    "label_group['Air50P'].agg(['mean','median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats for all clusters by varible \n",
    "label_group.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-silver",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#scaling data for plotting\n",
    "scaled_clusters = clusters.copy() #create a copy so that origonal not lost or modified \n",
    "scaled_clusters[['YearBuilt','Rating']] = minmax.fit_transform(scaled_clusters[['YearBuilt','Rating']]) #put in the cl_varibles or whatever to make it automated\n",
    "#scaled_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot with centroids\n",
    "ax=sns.scatterplot(y=scaled_clusters.columns[2],x= scaled_clusters.columns[1], data=scaled_clusters, hue= 'label')\n",
    "ax=sns.scatterplot(y=centroids[:,1],x=centroids[:,0], s=40, ec='black', legend=False, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot without centroids and  not scaled \n",
    "sns.scatterplot(y='YearBuilt',x= 'Air50P', data=clusters, hue= 'label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-tomorrow",
   "metadata": {},
   "source": [
    "Plot comparion of all variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-carroll",
   "metadata": {},
   "source": [
    "# Larger than 2D clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-gregory",
   "metadata": {},
   "source": [
    "## Data source \n",
    "The data represents a subset of the ERS records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "ers_sample_records = pd.read_csv(r\"C:\\Users\\owner\\Documents\\NRCan\\code\\practice\\InitialHousingData.csv\",)\n",
    "ers_sample_records \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-packet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ers_sample_records.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-neighborhood",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-clearing",
   "metadata": {},
   "source": [
    "Remove missing houses (rows) that have missing data in at least one cloumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-dodge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Drop the rows where at least one element is NA.\n",
    "cleaned_data = ers_sample_records.dropna()\n",
    "removed = len(ers_sample_records )-len(cleaned_data)\n",
    "#print amount of rows removed \n",
    "print('rows of data dropped:%d' %removed )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-agreement",
   "metadata": {},
   "source": [
    "Remove houses (rows) that have inconsistent data. The definition of inconsistent depends on the varible, ensure to change this area when using a new dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input bounds for each paramter\n",
    "filt_data = cleaned_data[cleaned_data['Air50P'] > 0]\n",
    "filt_data = filt_data[filt_data['Rating'] > 0]\n",
    "filt_data = filt_data[filt_data['YearBuilt'] > 0]\n",
    "filt_data = filt_data[filt_data['FloorArea'] > 0]\n",
    "filt_data = filt_data[filt_data['MainWallIns'] > 0]\n",
    "\n",
    "#print amount of rows removed \n",
    "removed = len(cleaned_data )-len(filt_data)\n",
    "print('rows of data dropped:%d' %removed )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-basketball",
   "metadata": {},
   "source": [
    "### Outiler Removal \n",
    "Local outlier factor algorithm detects the outliers and they are subsequently removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-artwork",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define the model\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20)\n",
    "lof_pred = lof.fit_predict(filt_data) \n",
    "\n",
    "#extract the negative outputs as the outliers.\n",
    "mask = lof_pred != -1\n",
    "\n",
    "#remove rows with outliers \n",
    "lof_data = filt_data[mask] #i dont think this works for sets with more than 1 varibale\n",
    "\n",
    "#print amount of points deleated\n",
    "\n",
    "outliers_rem = len(filt_data) - lof_data.shape[0]\n",
    "print ('Outliers removed: %d' %outliers_rem )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-theology",
   "metadata": {},
   "source": [
    "To visualize the outlier removal the parameters are scaled using standardscaler(z-score) so that they can be compared on the same axes then plotted with boxplots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-sierra",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#inintalize scaler \n",
    "scaled = StandardScaler()\n",
    "\n",
    "#scale filtered data in preparation for plotting \n",
    "scaled_filt_data = pd.DataFrame(scaled.fit_transform(filt_data))\n",
    "\n",
    "#plot boxplots before outlier removal\n",
    "\n",
    "scaled_filt_data.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-volleyball",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scale data with outliers removed in preparation for plotting\n",
    "\n",
    "scaled_parameters = scaled.fit_transform(lof_data)\n",
    "scaled_parameters = pd.DataFrame(scaled_parameters)\n",
    "\n",
    "#plot boxplots after outlier removal\n",
    "scaled_parameters.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-polyester",
   "metadata": {},
   "source": [
    "## Parameter selection \n",
    "The minimal amount of paramters that represent the building stock data effectively are selected to be clustered. The parameters that are not selected are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the column name of the selected parameters that will be held by the varibale parameters\n",
    "parameters = ['Air50P','Rating', 'YearBuilt', 'FloorArea', 'MainWallIns']\n",
    "# create a new data frame with parameters \n",
    "cl_data = lof_data[parameters]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-dairy",
   "metadata": {},
   "source": [
    " categorically filter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-collection",
   "metadata": {},
   "source": [
    "### Parameter scaling \n",
    "K-means is distance based so for it to consider all attributes as equal and produce unbiased resutls, they must all have the same scale. Minmax scaling rescales the data into a given range (0-1), this method was chosen from earlier 1D/2D clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalize scaler\n",
    "minmax = MinMaxScaler()\n",
    "#scale parameters\n",
    "scaled_parameters = minmax.fit_transform(cl_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-degree",
   "metadata": {},
   "source": [
    "## K-means clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-witness",
   "metadata": {},
   "source": [
    "###  Optmial cluster amout determination\n",
    "Davies Bouldin score is used to determine the optimal amount of clusters for the data. The DB score is calculated for a range of k-values (2-11) and the k-value that gives the lowest DB score is optimal. This is then used as an input for the k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "DB_score = []\n",
    "\n",
    "kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",\n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,\n",
    "}\n",
    "\n",
    "for k in range(2,11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(scaled_parameters)\n",
    "    pred_labels = kmeans.labels_\n",
    "    dav_score = davies_bouldin_score(scaled_parameters, pred_labels)\n",
    "    DB_score.append(dav_score)\n",
    "   \n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(2,11), DB_score)\n",
    "plt.xticks(range(2,11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"DB\")\n",
    "plt.show()\n",
    "\n",
    "#find min BD = optimum K\n",
    "\n",
    "#amount of clusters for DB\n",
    "min_index = DB_score.index(min(DB_score))\n",
    "DB_k =range(2,11)\n",
    "DB_k = DB_k[min_index]\n",
    "DB_k "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-berry",
   "metadata": {},
   "source": [
    "### Clustering \n",
    "The prepared data is now clustered using the opitmal amount of clusters found in the past step as the n_clusters argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters=DB_k,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "\n",
    "kmeans.fit(scaled_parameters)\n",
    "labels = pd.DataFrame(kmeans.labels_,columns=['label']) #creating a dataframe to house cluster labels for each house\n",
    "centroids = kmeans.cluster_centers_ #creating varible to hold centroid values (scaled) \n",
    "centroids_real = minmax.inverse_transform(centroids) #centriods with non-scaled values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-guyana",
   "metadata": {},
   "source": [
    "A new dataframe is created adding a column 'label' to show the rows corresponding cluster label decided during k-means clustering. This will allow the resulting paritions to be viewed easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_data_new_index = cl_data.reset_index(drop=True) #new index to allow concatination to line up properly \n",
    "#can concatanate along columns but ensure that indicies are the same\n",
    "clusters = pd.concat([labels,cl_data_new_index], axis = 1) \n",
    "#make varible for label groups\n",
    "label_group = clusters.groupby(['label']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-journalism",
   "metadata": {},
   "source": [
    "### Result statisics \n",
    "The statisics are a way to evalute the results numerically. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-pollution",
   "metadata": {},
   "source": [
    "The amount of objects (houses) in each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-creek",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_group.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-mouse",
   "metadata": {},
   "source": [
    "The mean(centroid), standard deviation, and range for each clusters and parameter. Smaller std mean tighter clusters (more compact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_group.agg(['mean','std','min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-honduras",
   "metadata": {},
   "source": [
    "The centriods values for each parameter. These represent the arcetypes characterisicts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-exercise",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#centroids \n",
    "\n",
    "centroids_real = pd.DataFrame(centroids_real, columns = [parameters])\n",
    "centroids_real2 = centroids_real.reset_index()\n",
    "centroids_real2= centroids_real2.rename(columns = {\"index\" : \"label\"})\n",
    "centroids_real2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-barrier",
   "metadata": {},
   "source": [
    "### Visualize results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-rally",
   "metadata": {},
   "source": [
    "Parallel coordinates colour coded by cluster label help to comprehend the distribution of each cluster and how distict the clusters are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-therapy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.parallel_coordinates(clusters, color=\"label\", \n",
    "                             color_continuous_scale=px.colors.diverging.Tealrose)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-capture",
   "metadata": {},
   "source": [
    "Box plots for each parameter also show how much overlap there is between clusters for each parameter and how compact each cluster is. IF the IQR of each cluster are not offset they are not distinct. The smaller the IQR and whiskers are the more compact the cluster is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up subplot\n",
    "fig, axes = plt.subplots(1, len(parameters), figsize=(18, 10))\n",
    "\n",
    "#use a while loop to plot each parameters distribution\n",
    "i=0\n",
    "while i != len(parameters):\n",
    "    \n",
    "    sns.boxplot(ax=axes[i], x=\"label\", y= parameters[i], data=clusters)\n",
    "    i=i+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
