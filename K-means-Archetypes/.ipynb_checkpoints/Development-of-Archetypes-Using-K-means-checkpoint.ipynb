{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "serious-deviation",
   "metadata": {},
   "source": [
    "# Creating housing archetypes using K-means method\n",
    "### Q4 2020-21\n",
    "\n",
    "## Introduction\n",
    "This workbook will ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from kneed import KneeLocator \n",
    "from sklearn.datasets import make_blobs \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from itertools import permutations\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-definition",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "The data represents a subset of ERS records used for initial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-blond",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ers_sample_records = pd.read_csv(r\"C:\\Users\\owner\\Documents\\NRCan\\code\\practice\\InitialHousingData.csv\",)\n",
    "ers_sample_records \n",
    "\n",
    "#original amount of obsevations/rows \n",
    "og_obs = ers_sample_records.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-workplace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ers_sample_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "ers_sample_records.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-world",
   "metadata": {},
   "source": [
    "## Database Preperation and Varibable Selection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-drink",
   "metadata": {},
   "source": [
    "### Select variables for clustering\n",
    "Placeholder for this process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Air50P selected for 1D clustering test\n",
    "cl_variables = ['MainWallIns']\n",
    "test_data = ers_sample_records[cl_variables]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-primary",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-stock",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove rows with blank values\n",
    "test_data_cleaned = test_data.dropna()\n",
    "#display how many rows removed\n",
    "test_data_cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-capital",
   "metadata": {},
   "source": [
    "###  Inconsistent data removal \n",
    "Dependant on the variable. Example: floor area could not be negavtive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-chicago",
   "metadata": {},
   "source": [
    "### Ensure all values are numerical\n",
    "K-means can only handle numerical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode catagorical variables \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-stuart",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-audience",
   "metadata": {},
   "source": [
    "K-means is based off finding the mean of clusters and since means are senstive to outliers so is k-means. If outliers are not delt with the they can have a large infulence in the clustering porcess that could result in poor partiotns that are not representaive of the data. Because of this outliers need to be removed properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-essence",
   "metadata": {},
   "source": [
    "Three different outlier removal techniques are tested: local outlier factor (LOF), z-score, and inter quartile range (IQR). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-bikini",
   "metadata": {},
   "source": [
    "The data will be interperated before and after outlier removal though box plots( and scatter?) and statisical measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-barcelona",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#box plot before outlier removal\n",
    "test_data_cleaned.boxplot() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-murray",
   "metadata": {},
   "source": [
    "#### Local outlier factor\n",
    "Local outlier factor (LOF) values identify an outlier based on the local neighborhood. It gives better results than the global approach to find outliers. A point will be considered as an outlier if it is at a small distance to the extremely dense cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-softball",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define the model\n",
    "\n",
    "lof = LocalOutlierFactor()\n",
    "lof_pred = lof.fit_predict(test_data_cleaned) \n",
    "\n",
    "#extract the negative outputs as the outliers.\n",
    "mask = lof_pred != -1\n",
    "\n",
    "#remove rows with outliers \n",
    "lof_data = test_data_cleaned[mask] #i dont think this works for sets with more than 1 varibale\n",
    "print([lof_data])\n",
    "\n",
    "#print amount of points deleated\n",
    "\n",
    "outliers_rem = og_obs - lof_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )\n",
    "\n",
    "#plot without after removal\n",
    "\n",
    "lof_data.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-mechanism",
   "metadata": {},
   "source": [
    "#### Z-score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find absolute value of z-score for each observation\n",
    "z = np.abs(stats.zscore(test_data_cleaned))\n",
    "\n",
    "#only keep rows in dataframe with all z-scores less than absolute value of 3 \n",
    "z_data = test_data_cleaned[(z<3).all(axis=1)]\n",
    "\n",
    "#print amount of outliers removed\n",
    "z_data\n",
    "\n",
    "outliers_rem = og_obs - z_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotafter outlier removal\n",
    "z_data.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-nicaragua",
   "metadata": {},
   "source": [
    "#### IQR\n",
    "The IQR can be used to identify outliers by defining limits on the sample values that are a factor k of the IQR below the 25th percentile or above the 75th percentile. The common value for the factor k is the value 1.5 (used below). A factor k of 3 or more can be used to identify values that are extreme outliers or “far outs” when described in the context of box and whisker plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find Q1, Q3, and interquartile range for each column\n",
    "Q1 =test_data_cleaned.quantile(q=.25)\n",
    "Q3 = test_data_cleaned.quantile(q=.75)\n",
    "IQR =test_data_cleaned.apply(stats.iqr)\n",
    "\n",
    "#only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3\n",
    "iqr_data = test_data_cleaned[~((test_data_cleaned < (Q1-1.5*IQR)) | (test_data_cleaned > (Q3+1.5*IQR))).any(axis=1)]\n",
    "\n",
    "#print amount of outliers removed\n",
    "\n",
    "outliers_rem = og_obs - iqr_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-breakdown",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot of IQR outlier removal\n",
    "iqr_data.boxplot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-concert",
   "metadata": {},
   "source": [
    " Mahalanobis distance - calc when multi variabale later\n",
    " https://www.statology.org/mahalanobis-distance-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-killer",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "Find the best mix of parameters for clustering. Parameters include oulier removal, scaling, initalization, and k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import validation metrics \n",
    "#could also use package validclust\n",
    "from sklearn.metrics import (silhouette_score, calinski_harabasz_score, davies_bouldin_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-basketball",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create sets of pre-processed data ready for clustering \n",
    "scaled_data_sets = [iqr_data,lof_data, z_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of scalers\n",
    "standard = StandardScaler()\n",
    "minimax = MinMaxScaler()\n",
    "scalers = [standard, minimax]\n",
    "#list of initalizers \n",
    "r = 'random'\n",
    "plus = 'k-means++' \n",
    "initalizer = [r, plus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets being transfromed with standard scaler and random init\n",
    "standard_rand_in=[]\n",
    "standard_rand_cal=[]\n",
    "standard_rand_dav=[]\n",
    "standard_rand_sil=[]\n",
    "\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[0].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elblw method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"random\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    standard_rand_in.append(kmeans.inertia_) # can use other validations too \n",
    "    \n",
    "    pred_labels = kmeans.labels_\n",
    "    \n",
    "    sil = silhouette_score(scaled_data_sets[x], pred_labels)\n",
    "    cal_score = calinski_harabasz_score(scaled_data_sets[x], pred_labels)\n",
    "    dav_score = davies_bouldin_score(scaled_data_sets[x], pred_labels)\n",
    "    \n",
    "    standard_rand_cal.append(cal_score)\n",
    "    standard_rand_dav.append(dav_score)\n",
    "    standard_rand_sil.append(sil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets being transfromed with mini scaler and random init\n",
    "mini_rand_in=[]\n",
    "mini_rand_cal=[]\n",
    "mini_rand_sil=[]\n",
    "mini_rand_dav=[]\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[1].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elblw method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"random\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "   \n",
    "    #validation metrics \n",
    "    \n",
    "    mini_rand_in.append(kmeans.inertia_) # can use other validations too \n",
    "   \n",
    "    pred_labels = kmeans.labels_\n",
    "    \n",
    "    sil = silhouette_score(scaled_data_sets[x], pred_labels)\n",
    "    cal_score = calinski_harabasz_score(scaled_data_sets[x], pred_labels)\n",
    "    dav_score = davies_bouldin_score(scaled_data_sets[x], pred_labels)\n",
    "\n",
    "    mini_rand_cal.append(cal_score)\n",
    "    mini_rand_sil.append(sil)\n",
    "    mini_rand_dav.append(dav_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets being transfromed with mini scaler and k++ init\n",
    "\n",
    "mini_plus_in=[]\n",
    "mini_plus_cal=[]\n",
    "mini_plus_sil=[]\n",
    "mini_plus_dav=[]\n",
    "\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[1].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elblw method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    mini_plus_in.append(kmeans.inertia_) # can use other validations too \n",
    "    \n",
    "    pred_labels = kmeans.labels_\n",
    "    \n",
    "    sil = silhouette_score(scaled_data_sets[x], pred_labels)\n",
    "    cal_score = calinski_harabasz_score(scaled_data_sets[x], pred_labels)\n",
    "    dav_score = davies_bouldin_score(scaled_data_sets[x], pred_labels)\n",
    "  \n",
    "    mini_plus_cal.append(cal_score)\n",
    "    mini_plus_sil.append(sil)\n",
    "    mini_plus_dav.append(dav_score)\n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets being transfromed with standard scaler and k++ init\n",
    "\n",
    "standard_plus_in=[]\n",
    "standard_plus_cal=[]\n",
    "standard_plus_dav=[]\n",
    "standard_plus_sil=[]\n",
    "\n",
    "\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[0].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elbow method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    standard_plus_in.append(kmeans.inertia_) # can use other validations too \n",
    "     \n",
    "    pred_labels = kmeans.labels_\n",
    "    \n",
    "    sil = silhouette_score(scaled_data_sets[x], pred_labels)\n",
    "    cal_score = calinski_harabasz_score(scaled_data_sets[x], pred_labels)\n",
    "    dav_score = davies_bouldin_score(scaled_data_sets[x], pred_labels)\n",
    "    \n",
    "    standard_plus_cal.append(cal_score)\n",
    "    standard_plus_dav.append(dav_score)\n",
    "    standard_plus_sil.append(sil)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-input",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#intertia results\n",
    "results_inertia = pd.DataFrame({ 'st rand':standard_rand_in, 'st plus':standard_plus_in,'min rand':mini_rand_in, 'min plus': mini_plus_in}, index= ['IRQ','LOF','Z-score'])\n",
    "results_inertia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DB results(Values closer to zero indicate a better partition.)\n",
    "results = pd.DataFrame({ 'st rand':standard_rand_dav, 'st plus':standard_plus_dav,'min rand':mini_rand_dav, 'min plus': mini_plus_dav}, index= ['IRQ','LOF','Z-score'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sil results (want higher=highly dense clustering)\n",
    "results = pd.DataFrame({ 'st rand':standard_rand_sil, 'st plus':standard_plus_sil,'min rand':mini_rand_sil, 'min plus': mini_plus_sil}, index= ['IRQ','LOF','Z-score'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cal results(want higher)\n",
    "results = pd.DataFrame({ 'st rand':standard_rand_cal, 'st plus':standard_plus_cal,'min rand':mini_rand_cal, 'min plus': mini_plus_cal}, index= ['IRQ','LOF','Z-score'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-guidance",
   "metadata": {},
   "source": [
    "## Result visualization \n",
    "Visulaizing the results of the nest method determined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-experiment",
   "metadata": {},
   "source": [
    "LOF outlier ommision, MinMax scaling, elbow method k determination, k-means++ initalization. Independant od Clustering performed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-monaco",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data\n",
    "data = lof_data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-contribution",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merging original data with cleaned data for plotting realtionships later\n",
    "lof_data_full = pd.merge(data, ers_sample_records, right_index=True, left_index =True) \n",
    "lof_data_full\n",
    "lof_data_full = lof_data_full.drop(columns=['MainWallIns_x'])\n",
    "#re-setting index for mergeing with cluster lables later\n",
    "data_all = lof_data_full\n",
    "data_all = data_all.reset_index(drop=True)\n",
    "data_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot before clustering to look for intutive clusters \n",
    "sns.pairplot(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling with MinMax scaler\n",
    "minmax = MinMaxScaler()\n",
    "scaled_features = minmax.fit_transform(data)\n",
    "#scaled feature into a data frame \n",
    "scaled_features = pd.DataFrame(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-tucson",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#compare Stats for scaled and unsclaed data\n",
    "#show scaled stats\n",
    "scaled_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show unscaled stats\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-accent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot histogram of scaled feaures \n",
    "scaled_features.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-junction",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# before scaling histogram\n",
    "data.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering\n",
    "\n",
    "kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\", \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "\n",
    "sse=[] #determine SSE for 1 to 11 clusters    \n",
    "for k in range(1,11): \n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "        \n",
    "#plot elbow method\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1,11), sse)\n",
    "plt.xticks(range(1,11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()\n",
    "        \n",
    "k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "clamount=k1.elbow\n",
    "    \n",
    "#cluster with k determined above \n",
    "    \n",
    "kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-belle",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a dataframe including all variabels and cluster labels \n",
    "kmeans.fit(scaled_features)\n",
    "labels = pd.DataFrame(kmeans.labels_,columns=['cluster label'])\n",
    "\n",
    "cluster1=pd.concat([labels,data_all], axis = 1) \n",
    "\n",
    "cluster1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-venture",
   "metadata": {},
   "source": [
    "Looking at clusters individually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-window",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cluster 0 dataframe\n",
    "cluster_0 = cluster1.loc[cluster1['cluster label'] == 0, ['MainWallIns_y']]\n",
    "cluster_0 = cluster_0.rename(columns={'MainWallIns_y': 'Cluster 0'})\n",
    "# can view boxplot and stats separate but easier to compare on same table see below \n",
    "#cluster_0.boxplot()\n",
    "#cluster_0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-pledge",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#cluster 1 dataframe\n",
    "cluster_1 = cluster1.loc[cluster1['cluster label'] == 1, ['MainWallIns_y']]\n",
    "cluster_1=cluster_1.rename(columns={'MainWallIns_y': 'Cluster 1'})\n",
    "#cluster_1.boxplot()\n",
    "#cluster_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-surgery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cluster 2 dataframe\n",
    "cluster_2 = cluster1.loc[cluster1['cluster label'] == 2, ['MainWallIns_y']]\n",
    "#cluster_2.boxplot()\n",
    "#cluster_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-festival",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cancatinate blox plot for each cluster to compare variation they capture for the certain vairbale \n",
    "#concatinate into a data frame \n",
    "all_clust = pd.concat([cluster_0,cluster_1, cluster_2], ignore_index=True, axis=1)\n",
    "#plot using pandas \n",
    "all_clust.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-respondent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#comparison of all cluster stats \n",
    "all_clust.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluser Centroids \n",
    "\n",
    "centroids = minmax.inverse_transform(kmeans.cluster_centers_) # transform scaled cenroids back\n",
    "\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-approach",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Scatter plot of custers not scaled \n",
    "sns.scatterplot('MainWallIns_y', 'YearBuilt', data=cluster1, hue= 'cluster label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot of scaled values \n",
    "\n",
    "# create a dataframe including all variabels and cluster labels \n",
    "kmeans.fit(scaled_features)\n",
    "labels = pd.DataFrame(kmeans.labels_,columns=['cluster label'])\n",
    "\n",
    "data_all_scaled = pd.DataFrame(minmax.fit_transform(data_all), columns = [ 'Air50P', 'Rating', 'YearBuilt', 'FloorArea',\n",
    "       'MainWallIns_y'])\n",
    "\n",
    "clusters_scaled = pd.concat([labels, data_all_scaled], axis = 1) \n",
    "sns.pairplot(clusters_scaled, hue = 'cluster label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-logic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot all varibles (not scaled) against eachother to find patterns in clusters  \n",
    "sns.pairplot(cluster1, hue='cluster label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-cabinet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#silhouette coefficent visualizer\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "\n",
    "visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n",
    "\n",
    "visualizer.fit(scaled_features)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "#In SilhouetteVisualizer plots, clusters with higher scores have wider silhouettes, but clusters \n",
    "#that are less cohesive will fall short of the average score across all clusters, which is plotted as a\n",
    "#vertical dotted red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inter cluster distance maps\n",
    "from yellowbrick.cluster import InterclusterDistance\n",
    "\n",
    "visualizer = InterclusterDistance(kmeans)\n",
    "\n",
    "visualizer.fit(scaled_features)    # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "#the closer to centers are in the visualization, the closer they are in the original feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-relay",
   "metadata": {},
   "source": [
    "## 2D clustering \n",
    "This area is completely separate from above 1D clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-typing",
   "metadata": {},
   "source": [
    "### Data preping \n",
    "Fist the data is retrieved and preped. Missing data is removed because k-means does not accept missing values and outliers are also removed because K-means is sensitive to outliers (a mean is easily influenced by extreme value). The preping will help acheive more robust clusting results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retreive data\n",
    "ers_sample_records = pd.read_csv(r\"C:\\Users\\owner\\Documents\\NRCan\\code\\practice\\InitialHousingData.csv\",)\n",
    "ers_sample_records \n",
    "\n",
    "#original amount of obsevations/rows \n",
    "og_obs = ers_sample_records.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-moral",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#retrive data from above\n",
    "ers_sample_records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-steal",
   "metadata": {},
   "source": [
    "Two variables are selected fo perform clustering on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-malaysia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#select variables for 2D clustering \n",
    "cl_variables = ['MainWallIns', 'Air50P' ] # for automation of code (hasn't been done yet)\n",
    "twoD_data = ers_sample_records[cl_variables] # a varible towD_data is created to hold clustering varibles specifcally for 2D analysis \n",
    "twoD_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-craft",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#review raw data stats \n",
    "twoD_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-replication",
   "metadata": {},
   "source": [
    "Cleaning data. Dropna from pandas is used to remove rows were at least on element is na and local outlier factor is applied for outlier detection. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-writing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Drop the rows where at least one element is NA.\n",
    "twoD_data_clean = twoD_data.dropna()\n",
    "removed = len(twoD_data)-len(twoD_data_clean)\n",
    "#print amount of rows removed \n",
    "print('rows of data dropped:%d' %removed )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-nebraska",
   "metadata": {},
   "source": [
    "Local outlier factor (LOF) values identify an outlier based on the local neighborhood. It gives better results than the global approach to find outliers. A point will be considered as an outlier if it is at a small distance to the extremely dense cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot varation with box plots before outlier removal to visualize outliers \n",
    "twoD_data.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-auction",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define the model\n",
    "\n",
    "lof = LocalOutlierFactor()\n",
    "lof_pred = lof.fit_predict(twoD_data_clean) \n",
    "\n",
    "#extract the negative outputs as the outliers.\n",
    "mask = lof_pred != -1\n",
    "\n",
    "#remove rows with outliers \n",
    "lof_data = twoD_data_clean[mask] #i dont think this works for sets with more than 1 varibale\n",
    "\n",
    "#print amount of points deleated\n",
    "\n",
    "outliers_rem = og_obs - lof_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )\n",
    "\n",
    "#plot without after removal to visualize outlier removal\n",
    "\n",
    "lof_data.boxplot() \n",
    "\n",
    "#should scale and plot as they have very differnt scales "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-cycling",
   "metadata": {},
   "source": [
    "Also can visualize outlier remvial though a scatter plot. The red values are the outliers that have been removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of removed outliers for plotting \n",
    "from numpy import where\n",
    "lofs_index = where(lof_pred==-1) \n",
    "#Filter_df  = twoD_data_clean[twoD_data_clean.index.isin(lofs_index)]\n",
    "outliers =twoD_data_clean.loc[lofs_index] #datarfame of outliers \n",
    "#could write a check here: len of oultleris == outliers rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-reserve",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot removed outliers in red\n",
    "plt.scatter(twoD_data_clean['MainWallIns'], twoD_data_clean['Air50P'])\n",
    "plt.scatter(outliers['MainWallIns'],outliers['Air50P'], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-manchester",
   "metadata": {},
   "source": [
    "Compare cleaned data stats to raw data to confirm the data is still meaningful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-minimum",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#stats after outlier removal \n",
    "lof_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "twoD_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-partnership",
   "metadata": {},
   "source": [
    "### Scaling \n",
    "k-means is distance based so for it to consider all attributes as equal and produce unbiased resutls, they must all have the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-forwarding",
   "metadata": {},
   "source": [
    "Min-max scaling rescales the data into a given range, in this case 0-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()\n",
    "scaled_features = minmax.fit_transform(lof_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-miami",
   "metadata": {},
   "source": [
    "### Clustering \n",
    "In this step the preped data is clustered with the k-means algorithm. K-means++ is used for inialization and the amount of clusters is determined with internal validity measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-pension",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-buddy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "# Instantiate the clustering model and visualizer\n",
    "# within cluster SSe (aka distortion)\n",
    "#Distortion is the average of the euclidean squared distance from the centroid of the respective clusters. \n",
    "#Inertia is the sum of squared distances of samples to their closest cluster centre\n",
    "model = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "\n",
    "#plot elbow method with distortion score\n",
    "\n",
    "visualizer = KElbowVisualizer(model, k=(1,10))\n",
    "\n",
    "visualizer.fit(lof_data)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "\n",
    "#plot elbow method Calinski Harabasz \n",
    "\n",
    "visualizer = KElbowVisualizer(model, k=(2,10),metric='calinski_harabasz')\n",
    "\n",
    "visualizer.fit(lof_data)        # Fit the data to the visualizer\n",
    "visualizer.show()    \n",
    "\n",
    "#plot elbow method with silhouette score \n",
    "\n",
    "visualizer = KElbowVisualizer(model, k=(2,10),metric='silhouette')\n",
    "\n",
    "visualizer.fit(lof_data)        # Fit the data to the visualizer\n",
    "visualizer.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot elbow method with interia \n",
    "\n",
    "sse=[] #determine SSE for 1 to 11 clusters\n",
    "kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",\n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,\n",
    "}\n",
    "\n",
    "for k in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(lof_data)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1,11), sse)\n",
    "plt.xticks(range(1,11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the elbow/k\n",
    "k1 = KneeLocator(\n",
    "    range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "k1.elbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-culture",
   "metadata": {},
   "source": [
    "After the optimal k vlaue the clustering algortihm is ran once more and resulting clusters can be analyzed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-coach",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#final k-means arguments \n",
    "kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters=6,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "kmeans.fit(scaled_features)\n",
    "labels = pd.DataFrame(kmeans.labels_,columns=['label']) #creating a dataframe to house cluster labels for each house\n",
    "centroids = kmeans.cluster_centers_ #creating varible to hold centroid values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-theory",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lof_data_new_index = lof_data.reset_index(drop=True) #new index to allow concatination to line up properly \n",
    "\n",
    "clusters = pd.concat([labels,lof_data_new_index], axis = 1) #can concatanate along columns but enssure that indicies are the same\n",
    "#using groupby to compare clustering results \n",
    "label_group = clusters.groupby(['label']) #make varible for lable groups\n",
    "\n",
    "label_group.get_group(1) #retrieve data frame of group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using group by to get median and means(cenroids) of the resulting clusters \n",
    "label_group['Air50P'].agg(['mean','median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "#geting mean and median for just one cluster \n",
    "label_group['MainWallIns'].agg(['mean','median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats for all clusters by varible \n",
    "label_group.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-jordan",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#scaling data for plotting\n",
    "scaled_clusters = clusters.copy() #create a copy so that origonal not lost or modified \n",
    "scaled_clusters[['MainWallIns','Air50P']] = minmax.fit_transform(scaled_clusters[['MainWallIns','Air50P']]) #put in the cl_varibles or whatever to make it automated\n",
    "scaled_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#centroids values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot with centroids\n",
    "ax=sns.scatterplot(x='MainWallIns',y= 'Air50P', data=scaled_clusters, hue= 'label')\n",
    "ax=sns.scatterplot(y=centroids[:,1],x=centroids[:,0], s=20, ec='black', legend=False, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot without centroids and  not scaled \n",
    "sns.scatterplot(x='MainWallIns',y= 'Air50P', data=clusters, hue= 'label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-resident",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-tokyo",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
