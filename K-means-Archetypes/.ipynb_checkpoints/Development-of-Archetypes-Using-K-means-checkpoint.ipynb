{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bigger-olive",
   "metadata": {},
   "source": [
    "# Creating housing archetypes using K-means method\n",
    "### Q4 2020-21\n",
    "\n",
    "## Introduction\n",
    "This workbook will demonstrate the use of k-means clustering for housing archetype creation. An optimal clustering process is developed on a test dataset though incremental clustering, starting with 1D, then 2D then multi-dimensional clustering. In each increment of clustering different methods are tested and new insight into how these methods perform are found though visualization and internal clustering indices. These findings are then carried on an applied in the next increment of clustering analysis. In the final increment of multi-dimensional clustering the process developed should produce acceptable results: a set of hosing archetypes that are representative of the dataset analyzed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-owner",
   "metadata": {},
   "source": [
    "### Notes \n",
    "#### \n",
    "The random state hyper-parameter of k-means is set to a value (42) to produce replicable results during testing; in practice this should be set to 'none'.\n",
    "#### \n",
    "K-value range of 2-11 clusters was predetermined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-glass",
   "metadata": {},
   "source": [
    "### Importing packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from numpy import where\n",
    "\n",
    "from scipy import stats\n",
    "from kneed import KneeLocator \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from sklearn.metrics import (silhouette_score, calinski_harabasz_score, davies_bouldin_score)\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from yellowbrick.cluster import InterclusterDistance\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "import plotly.express as px\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-definition",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "The data represents a subset of ERS records used for initial analysis. This source is used for each increment of clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-blond",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ers_sample_records = pd.read_csv(r\"C:\\Users\\owner\\Documents\\NRCan\\code\\practice\\InitialHousingData.csv\",)\n",
    "ers_sample_records \n",
    "\n",
    "#original amount of obsevations/rows \n",
    "og_obs = ers_sample_records.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "ers_sample_records.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-maple",
   "metadata": {},
   "source": [
    "# 1D Clustering \n",
    "Outlier detection methods, normalization methods, and k-means initialization methods were tested and compared with resulting internal indices and scatter plots after cluster analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-drink",
   "metadata": {},
   "source": [
    "## Select Variables\n",
    "Select a single variable to perform a cluster analysis on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input variable for clustering as cl_variables\n",
    "cl_variables = ['FloorArea']\n",
    "# the data is filtered to only include the previously selected variable\n",
    "test_data = ers_sample_records[cl_variables]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-logic",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Rows (houses) with missing data and unrealistic values are removed. Outliers are detected using three different methods and are subsequently removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-primary",
   "metadata": {},
   "source": [
    "##Missing values\n",
    "Dropna from pandas is used to remove rows were at least on element is na. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-stock",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove rows with blank values\n",
    "test_data_cleaned = test_data.dropna()\n",
    "#display how many rows removed by comparing to ers_sample_records stats\n",
    "test_data_cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-capital",
   "metadata": {},
   "source": [
    "###  Inconsistent data removal \n",
    "Dependent on the variable. Example: floor area could not be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_data = test_data_cleaned[test_data_cleaned > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-chicago",
   "metadata": {},
   "source": [
    "### Ensure all values are numerical\n",
    "K-means can only handle numerical values. \n",
    "None of the variables were categorical so this step was not applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-stuart",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-audience",
   "metadata": {},
   "source": [
    "K-means is based off finding the mean of clusters and since means are sensitive to outliers so is k-means. If outliers are not delt with the they can have a large influence in the clustering process that could result in poor partitions that are not representative of the data. Because of this outliers need to be removed properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-seeking",
   "metadata": {},
   "source": [
    "Three different outlier removal techniques are tested: local outlier factor (LOF), z-score, and inter quartile range (IQR). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-corner",
   "metadata": {},
   "source": [
    "The data can be interpreted before and after outlier removal though box plots and statistical measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-barcelona",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#boxplot before outlier removal\n",
    "filt_data.boxplot() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-murray",
   "metadata": {},
   "source": [
    "#### Local outlier factor\n",
    "Local outlier factor (LOF) values identify an outlier based on the local neighborhood. It gives better results than the global approach to find outliers. A point will be considered as an outlier if it is at a small distance to the extremely dense cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-softball",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Normalize data before lof \n",
    "minmax = MinMaxScaler()\n",
    "scaled_data_lof = minmax.fit_transform(filt_data)\n",
    "\n",
    "#define the model\n",
    "\n",
    "lof = LocalOutlierFactor()\n",
    "lof_pred = lof.fit_predict(scaled_data_lof) \n",
    "\n",
    "#extract the negative outputs as the outliers.\n",
    "mask = lof_pred != -1\n",
    "\n",
    "#remove rows with outliers \n",
    "lof_data = filt_data[mask]\n",
    "print([lof_data])\n",
    "\n",
    "#print amount of points deleted\n",
    "\n",
    "outliers_rem = og_obs - lof_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )\n",
    "\n",
    "#plot without after removal\n",
    "\n",
    "lof_data.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-mechanism",
   "metadata": {},
   "source": [
    "#### Z-score \n",
    "The z-score method labels an object an outlier depending on its distance from the mean. The distance is measured using standard deviations and is based on the assumption that the data has a gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find absolute value of z-score for each observation\n",
    "z = np.abs(stats.zscore(filt_data))\n",
    "\n",
    "#only keep rows in dataframe with all z-scores less than absolute value of 3 \n",
    "z_data = test_data_cleaned[(z<3).all(axis=1)]\n",
    "\n",
    "#print amount of outliers removed\n",
    "z_data\n",
    "\n",
    "outliers_rem = og_obs - z_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot after outlier removal\n",
    "z_data.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-nicaragua",
   "metadata": {},
   "source": [
    "#### IQR\n",
    "The IQR can be used to identify outliers by defining limits on the sample values that are a factor k of the IQR below the 25th percentile or above the 75th percentile. The common value for the factor k is the value 1.5 (used below). A factor k of 3 or more can be used to identify values that are extreme outliers or “far outs” when described in the context of box and whisker plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find Q1, Q3, and interquartile range for each column\n",
    "Q1 =filt_data.quantile(q=.25)\n",
    "Q3 = filt_data.quantile(q=.75)\n",
    "IQR =filt_data.apply(stats.iqr)\n",
    "\n",
    "#only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3\n",
    "iqr_data = filt_data[~((filt_data < (Q1-1.5*IQR)) | (filt_data > (Q3+1.5*IQR))).any(axis=1)]\n",
    "\n",
    "#print amount of outliers removed\n",
    "\n",
    "outliers_rem = og_obs - iqr_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-breakdown",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot of IQR outlier removal\n",
    "iqr_data.boxplot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-killer",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "The clustering analysis is performed on each set of preprocessed data (3 sets different by outlier detection methods) using a combination of methods for Find the best combination of parameters for clustering. Parameters include outlier removal, scaling, initialization, and optimal amount of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-basketball",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create sets of pre-processed data ready for clustering \n",
    "preprocessed_data_sets = [iqr_data,lof_data, z_data]\n",
    "#list of scalers\n",
    "standard = StandardScaler()\n",
    "minimax = MinMaxScaler()\n",
    "scalers = [standard, minimax]\n",
    "#list of initializers  \n",
    "r = 'random'\n",
    "plus = 'k-means++' \n",
    "initalizer = [r, plus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets being transformed with standard scaler and random initialization\n",
    "standard_rand_in=[]\n",
    "standard_rand_cal=[]\n",
    "standard_rand_dav=[]\n",
    "standard_rand_sil=[]\n",
    "\n",
    "for x in range(len(preprocessed_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[0].fit_transform(preprocessed_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow method\n",
    "    \n",
    "    sse=[] #determine SSE(inertia) for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"random\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clamount  \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    standard_rand_in.append(kmeans.inertia_) \n",
    "    \n",
    "    pred_labels = kmeans.labels_\n",
    "    \n",
    "    sil = silhouette_score(preprocessed_data_sets[x], pred_labels)\n",
    "    cal_score = calinski_harabasz_score(preprocessed_data_sets[x], pred_labels)\n",
    "    dav_score = davies_bouldin_score(preprocessed_data_sets[x], pred_labels)\n",
    "    \n",
    "    standard_rand_cal.append(cal_score)\n",
    "    standard_rand_dav.append(dav_score)\n",
    "    standard_rand_sil.append(sil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets being transformed with minmax scaler and random initialization\n",
    "mini_rand_in=[]\n",
    "mini_rand_cal=[]\n",
    "mini_rand_sil=[]\n",
    "mini_rand_dav=[]\n",
    "for x in range(len(preprocessed_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[1].fit_transform(preprocessed_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elblw method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"random\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "   \n",
    "    #validation metrics \n",
    "    \n",
    "    mini_rand_in.append(kmeans.inertia_) # can use other validations too \n",
    "   \n",
    "    pred_labels = kmeans.labels_\n",
    "    \n",
    "    sil = silhouette_score(preprocessed_data_sets[x], pred_labels)\n",
    "    cal_score = calinski_harabasz_score(preprocessed_data_sets[x], pred_labels)\n",
    "    dav_score = davies_bouldin_score(preprocessed_data_sets[x], pred_labels)\n",
    "\n",
    "    mini_rand_cal.append(cal_score)\n",
    "    mini_rand_sil.append(sil)\n",
    "    mini_rand_dav.append(dav_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets being transformed with minmax scaler and k-means++ initialization\n",
    "\n",
    "mini_plus_in=[]\n",
    "mini_plus_cal=[]\n",
    "mini_plus_sil=[]\n",
    "mini_plus_dav=[]\n",
    "\n",
    "for x in range(len(preprocessed_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[1].fit_transform(preprocessed_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow method\n",
    "    \n",
    "    sse=[] #determine SSE(inertia) for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined k amount \n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clamount \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    mini_plus_in.append(kmeans.inertia_) \n",
    "    \n",
    "    pred_labels = kmeans.labels_\n",
    "    \n",
    "    sil = silhouette_score(preprocessed_data_sets[x], pred_labels)\n",
    "    cal_score = calinski_harabasz_score(preprocessed_data_sets[x], pred_labels)\n",
    "    dav_score = davies_bouldin_score(preprocessed_data_sets[x], pred_labels)\n",
    "  \n",
    "    mini_plus_cal.append(cal_score)\n",
    "    mini_plus_sil.append(sil)\n",
    "    mini_plus_dav.append(dav_score)\n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets being transformed with standard scaler and k-means++ initialization\n",
    "\n",
    "standard_plus_in=[]\n",
    "standard_plus_cal=[]\n",
    "standard_plus_dav=[]\n",
    "standard_plus_sil=[]\n",
    "\n",
    "\n",
    "for x in range(len(preprocessed_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[0].fit_transform(preprocessed_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clamount\n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    standard_plus_in.append(kmeans.inertia_)  \n",
    "     \n",
    "    pred_labels = kmeans.labels_\n",
    "    \n",
    "    sil = silhouette_score(preprocessed_data_sets[x], pred_labels)\n",
    "    cal_score = calinski_harabasz_score(preprocessed_data_sets[x], pred_labels)\n",
    "    dav_score = davies_bouldin_score(preprocessed_data_sets[x], pred_labels)\n",
    "    \n",
    "    standard_plus_cal.append(cal_score)\n",
    "    standard_plus_dav.append(dav_score)\n",
    "    standard_plus_sil.append(sil)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-study",
   "metadata": {},
   "source": [
    "### Internal Index Results \n",
    "The resulting internal index scores are summarized in tables below. A larger score for inertia, silhouette, and Calinski-Harabasz indicate better clustering, while a smaller Davies-Bouldin index indicates better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-input",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#intertia results\n",
    "results_inertia = pd.DataFrame({ 'st rand':standard_rand_in, 'st plus':standard_plus_in,'min rand':mini_rand_in, 'min plus': mini_plus_in}, index= ['IRQ','LOF','Z-score'])\n",
    "results_inertia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "#davies bouldin results\n",
    "results = pd.DataFrame({ 'st rand':standard_rand_dav, 'st plus':standard_plus_dav,'min rand':mini_rand_dav, 'min plus': mini_plus_dav}, index= ['IRQ','LOF','Z-score'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#silhouette results \n",
    "results = pd.DataFrame({ 'st rand':standard_rand_sil, 'st plus':standard_plus_sil,'min rand':mini_rand_sil, 'min plus': mini_plus_sil}, index= ['IRQ','LOF','Z-score'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calinski harabasz results\n",
    "results = pd.DataFrame({ 'st rand':standard_rand_cal, 'st plus':standard_plus_cal,'min rand':mini_rand_cal, 'min plus': mini_plus_cal}, index= ['IRQ','LOF','Z-score'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-guidance",
   "metadata": {},
   "source": [
    "### Result Visualization \n",
    "Based off the resulting internal indices the optimal clustering process included LOF outlier detection, Minmax scaling, inertia (elbow method) for k-value determination, k-means++ initialization. The clusters formed from this method are visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-contribution",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data\n",
    "data = lof_data\n",
    "\n",
    "#merging original data with cleaned data for plotting relationships later\n",
    "lof_data_full = pd.merge(data, ers_sample_records, right_index=True, left_index =True) \n",
    "lof_data_full\n",
    "lof_data_full = lof_data_full.drop(columns=['Rating_x'])\n",
    "\n",
    "#re-setting index for merging with cluster labels later\n",
    "data_all = lof_data_full\n",
    "data_all = data_all.reset_index(drop=True)\n",
    "data_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot before clustering to look for intuitive clusters \n",
    "sns.pairplot(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling with MinMax scaler\n",
    "minmax = MinMaxScaler()\n",
    "scaled_features = minmax.fit_transform(data)\n",
    "\n",
    "#scaled feature into a data frame \n",
    "scaled_features = pd.DataFrame(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-accent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot histogram of scaled features \n",
    "scaled_features.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-junction",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# before scaling histogram\n",
    "data.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering\n",
    "\n",
    "kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\", \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "\n",
    "sse=[] #determine SSE for 1 to 11 clusters    \n",
    "for k in range(1,11): \n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "        \n",
    "#plot elbow method\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1,11), sse)\n",
    "plt.xticks(range(1,11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()\n",
    "        \n",
    "k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "clamount=k1.elbow\n",
    "    \n",
    "#cluster with k determined above \n",
    "    \n",
    "kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-belle",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a dataframe including all variables and cluster labels \n",
    "kmeans.fit(scaled_features)\n",
    "labels = pd.DataFrame(kmeans.labels_,columns=['cluster label'])\n",
    "\n",
    "cluster1=pd.concat([labels,data_all], axis = 1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-venture",
   "metadata": {},
   "source": [
    "Evaluate resulting clusters stats. Stats show the distribution of the amount of houses each cluster contains and the variation in the variable used for clustering as well as the other variables the test data included. Optimal clustering can be represented by a smaller std and means/centroids that are dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-festival",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cluster 1 dataframe\n",
    "cluster_0 = cluster1.loc[cluster1['cluster label'] == 0, ['MainWallIns_y']]\n",
    "cluster_0 = cluster_0.rename(columns={'MainWallIns_y': 'Cluster 0'})\n",
    "\n",
    "#cluster 1 dataframe\n",
    "cluster_1 = cluster1.loc[cluster1['cluster label'] == 1, ['MainWallIns_y']]\n",
    "cluster_1=cluster_1.rename(columns={'MainWallIns_y': 'Cluster 1'})\n",
    "\n",
    "#cluster 1 dataframe\n",
    "cluster_2 = cluster1.loc[cluster1['cluster label'] == 2, ['MainWallIns_y']]\n",
    "\n",
    "#concatinate boxplot for each cluster to compare variation they capture for the certain variable \n",
    "#concatinate into a data frame\n",
    "all_clust = pd.concat([cluster_0,cluster_1, cluster_2], ignore_index=True, axis=1)\n",
    "\n",
    "#comparison of all cluster stats \n",
    "all_clust.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-sacrifice",
   "metadata": {},
   "source": [
    "Box plots are a visual representation of the stats. Box plots for each parameter also show how much overlap there is between clusters for each parameter and how compact each cluster is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot using pandas \n",
    "all_clust.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-debate",
   "metadata": {},
   "source": [
    "Cluster centroid values. These are the mean of each cluster and the values that would represent the synthetic housing archetype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster Centroids \n",
    "\n",
    "centroids = minmax.inverse_transform(kmeans.cluster_centers_) # transform scaled cenroids back\n",
    "\n",
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-connection",
   "metadata": {},
   "source": [
    "Visualize cluster results through scatter plots silhouette shadow plots and inter-cluster distance plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-approach",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Scatter plot of clusters not scaled\n",
    "sns.scatterplot('MainWallIns_y', 'YearBuilt', data=cluster1, hue= 'cluster label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot of scaled values \n",
    "\n",
    "#create a dataframe including all variables and cluster labels \n",
    "kmeans.fit(scaled_features)\n",
    "labels = pd.DataFrame(kmeans.labels_,columns=['cluster label'])\n",
    "\n",
    "data_all_scaled = pd.DataFrame(minmax.fit_transform(data_all), columns = [ 'Air50P', 'Rating', 'YearBuilt', 'FloorArea',\n",
    "       'MainWallIns_y'])\n",
    "\n",
    "clusters_scaled = pd.concat([labels, data_all_scaled], axis = 1) \n",
    "sns.pairplot(clusters_scaled, hue = 'cluster label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-logic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot all variables (not scaled) against each other to find patterns in clusters  \n",
    "sns.pairplot(cluster1, hue='cluster label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-cabinet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initiate the clustering model and visualizer\n",
    "\n",
    "visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n",
    "\n",
    "visualizer.fit(scaled_features)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "#In SilhouetteVisualizer plots, clusters with higher scores have wider silhouettes, but clusters \n",
    "#that are less cohesive will fall short of the average score across all clusters, which is plotted as a\n",
    "#vertical dotted red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inter cluster distance maps\n",
    "visualizer = InterclusterDistance(kmeans)\n",
    "\n",
    "visualizer.fit(scaled_features)    # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "#the closer to centers are in the visualization, the closer they are in the original feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-cross",
   "metadata": {},
   "source": [
    "# 2D Clustering \n",
    "In this section two variables were clustered using the optimal clustering methods determined in 1D clustering: LOF outlier detection, Minmax scaling, and k-means++ initialization. Methods for optimal amount of clusters were tested and evaluated though "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-links",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Fist the data is retrieved and prepared. Missing data is removed because k-means does not accept missing values and outliers are also removed because K-means is sensitive to outliers (a mean is easily influenced by extreme value). The preparing will help achieve more robust clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ccreate a variable hold the amount of houses in the test dataset \n",
    "og_obs = ers_sample_records.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-employer",
   "metadata": {},
   "source": [
    "### Variable Selection \n",
    "Two variables are selected to perform clustering on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-judges",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#select variables for 2D clustering \n",
    "cl_variables = ['YearBuilt', 'Air50P'] \n",
    "\n",
    "# a variable towD_data is created to hold clustering variables specifically for 2D analysis \n",
    "twoD_data = ers_sample_records[cl_variables] \n",
    "twoD_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-unemployment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#review raw data stats \n",
    "twoD_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-valve",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "Dropna from pandas is used to remove rows were at least on element is na and local outlier factor is applied for outlier detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-battery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Drop the rows where at least one element is NA.\n",
    "twoD_data_clean = twoD_data.dropna()\n",
    "removed = len(twoD_data)-len(twoD_data_clean)\n",
    "\n",
    "#print amount of rows removed \n",
    "print('rows of data dropped:%d' %removed )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_data = twoD_data_clean[twoD_data_clean['YearBuilt'] > 0]\n",
    "filt_data = filt_data[twoD_data_clean['Air50P'] > 0]\n",
    "filt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-beauty",
   "metadata": {},
   "source": [
    "### Scaling \n",
    "k-means is distance based so for it to consider all attributes as equal and produce unbiased results, they must all have the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-split",
   "metadata": {},
   "source": [
    "Min-max scaling rescales the data into a given range, in this case 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data before using LOF\n",
    "minmax = MinMaxScaler()\n",
    "twoD_data_scaled = minmax.fit_transform(filt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-embassy",
   "metadata": {},
   "source": [
    "### Outlier removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-cursor",
   "metadata": {},
   "source": [
    "Local outlier factor (LOF) values identify an outlier based on the local neighborhood. It gives better results than the global approach to find outliers. A point will be considered as an outlier if it is at a small distance to the extremely dense cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot variation with box plots before outlier removal to visualize outliers \n",
    "filt_data.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-springfield",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define the model\n",
    "\n",
    "lof = LocalOutlierFactor()\n",
    "lof_pred = lof.fit_predict(twoD_data_scaled)  \n",
    "\n",
    "#extract the negative outputs as the outliers.\n",
    "mask = lof_pred != -1\n",
    "\n",
    "#remove rows with outliers \n",
    "lof_data = filt_data[mask] \n",
    "\n",
    "#print amount of points deleated\n",
    "\n",
    "outliers_rem = og_obs - lof_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )\n",
    "\n",
    "#plot without after removal to visualize outlier removal\n",
    "lof_data = pd.DataFrame(lof_data, columns = cl_variables)\n",
    "lof_data.boxplot() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-advance",
   "metadata": {},
   "source": [
    "Also can visualize outlier removal though a scatter plot. The red points are the houses determined outliers that have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of removed outliers for plotting \n",
    "lofs_index = where(lof_pred==-1) \n",
    "#Filter_df  = twoD_data_clean[twoD_data_clean.index.isin(lofs_index)]\n",
    "outliers =twoD_data_clean.loc[lofs_index] #datarfame of outliers \n",
    "plt.scatter(twoD_data_clean['Air50P'], twoD_data_clean['YearBuilt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-framing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot removed outliers in red\n",
    "plt.scatter(twoD_data_clean['Air50P'], twoD_data_clean['YearBuilt'])\n",
    "plt.scatter(outliers['Air50P'],outliers['YearBuilt'], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-portrait",
   "metadata": {},
   "source": [
    "Compare cleaned data stats to raw data to confirm the data is still meaningful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-dublin",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#stats after outlier removal \n",
    "lof_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "twoD_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-handle",
   "metadata": {},
   "source": [
    "Normalize with minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()\n",
    "lof_data = minmax.fit_transform(lof_data)\n",
    "lof_data = pd.DataFrame(lof_data, columns = cl_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-nutrition",
   "metadata": {},
   "source": [
    "## Clustering \n",
    "In this step the prepared data is clustered with the k-means algorithm. K-means++ is used for initialization and the amount of clusters is determined with internal validity measures: Calinski Harabasz, silhouette, inertia, Davies-Bouldin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-saint",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model used for CH and Silhouette \n",
    "model = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "\n",
    "#plot Calinski Harabasz \n",
    "\n",
    "visualizer = KElbowVisualizer(model, k=(2,10),metric='calinski_harabasz')\n",
    "\n",
    "visualizer.fit(lof_data)        # Fit the data to the visualizer\n",
    "visualizer.show()    \n",
    "\n",
    "#plot silhouette score \n",
    "lof\n",
    "visualizer = KElbowVisualizer(model, k=(2,10),metric='silhouette')\n",
    "\n",
    "visualizer.fit(lof_data)        # Fit the data to the visualizer\n",
    "visualizer.show()  \n",
    "\n",
    "# plot elbow method with inertia/WSS/SSE\n",
    "\n",
    "sse=[] #determine SSE for 1 to 11 clusters, SSE = WSS\n",
    "kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",\n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,\n",
    "}\n",
    "\n",
    "for k in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(lof_data)\n",
    "    sse.append(kmeans.inertia_)\n",
    " \n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1,11), sse)\n",
    "plt.xticks(range(1,11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()\n",
    "\n",
    "#find the elbow \n",
    "k1 = KneeLocator(\n",
    "    range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "k1.elbow\n",
    "\n",
    "# plor Db score\n",
    "DB_score = []\n",
    "\n",
    "kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",\n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,\n",
    "}\n",
    "\n",
    "for k in range(2,11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(lof_data)\n",
    "    pred_labels = kmeans.labels_\n",
    "    dav_score = davies_bouldin_score(lof_data, pred_labels)\n",
    "    DB_score.append(dav_score)\n",
    "   \n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(2,11), DB_score)\n",
    "plt.xticks(range(2,11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"DB\")\n",
    "plt.show()\n",
    "\n",
    "#find min BD = optimum K\n",
    "min_index = DB_score.index(min(DB_score))\n",
    "DB_k =range(2,11)\n",
    "DB_k = DB_k[min_index]\n",
    "DB_k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1.elbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-panama",
   "metadata": {},
   "source": [
    "Determining the optimal amount of clusters (k-value) is hard as there is no 'correct' amount, in this case the k-values found from each internal index where used in a cluster analysis(below) then they were visualized using scatter plots and their stats were evaluated. \n",
    "\n",
    "After the optimal k value is found the clustering algorithm is ran once more and resulting clusters can be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-synthesis",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#final k-means arguments \n",
    "kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters=3, #change k value here\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "kmeans.fit(lof_data)\n",
    "labels = pd.DataFrame(kmeans.labels_,columns=['label']) #creating a dataframe to house cluster labels for each house\n",
    "centroids = kmeans.cluster_centers_ #creating varible to hold centroid values \n",
    "centroids_real = minmax.inverse_transform(centroids) #also mean of cluster\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_data_new_index = lof_data.reset_index(drop=True) #new index to allow concatination to line up properly \n",
    "\n",
    "clusters = pd.concat([labels,lof_data_new_index], axis = 1)\n",
    "\n",
    "#using groupby to compare clustering results \n",
    "label_group = clusters.groupby(['label']) #make variable for label groups\n",
    "\n",
    "label_group.get_group(1) #retrieve data frame of group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unscaled data \n",
    "lof_data_real = minmax.inverse_transform(lof_data)\n",
    "lof_data_real = pd.DataFrame(lof_data_real, columns = cl_variables )\n",
    "lof_data_real_new_index = lof_data_real.reset_index(drop=True) #new index to allow concatination to line up properly \n",
    "\n",
    "clusters_real = pd.concat([labels,lof_data_real_new_index], axis = 1)\n",
    "\n",
    "#using groupby to compare clustering results \n",
    "label_group_real = clusters_real.groupby(['label']) #make variable for label groups\n",
    "\n",
    "label_group_real.get_group(1) #retrieve data frame of group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_group_real.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-privacy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#using group by to get median and means(centroids) of the resulting clusters\n",
    "label_group['YearBuilt'].agg(['mean','median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#geting mean and median for just one cluster \n",
    "label_group['Air50P'].agg(['mean','median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats for all clusters by variable\n",
    "label_group.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-silver",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#scaling data for plotting\n",
    "scaled_clusters = clusters.copy() #create a copy so that original not lost or modified \n",
    "scaled_clusters[['Air50P', 'YearBuilt']] = minmax.fit_transform(scaled_clusters[['Air50P', 'YearBuilt']]) #put in the cl_varibles or whatever to make it automated\n",
    "#scaled_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot with centroids\n",
    "ax=sns.scatterplot(x=scaled_clusters.columns[2],y= scaled_clusters.columns[1], data=scaled_clusters, hue= 'label')\n",
    "ax=sns.scatterplot(x=centroids[:,1],y=centroids[:,0], s=40, ec='black', legend=False, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot without centroids and  not scaled \n",
    "sns.scatterplot(y='YearBuilt',x= 'Air50P', data=clusters, hue= 'label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-carroll",
   "metadata": {},
   "source": [
    "# Multidimensional Clustering \n",
    "New additions are unrealistic values are removed, option to pre-filter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-baker",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Rows with missing data and unrealistic values are removed. Outliers are detected using LOF and are subsequently removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-morris",
   "metadata": {},
   "source": [
    "Rename varibales to more intuitive names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-interim",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#rename columns \n",
    "ers_sample_records = ers_sample_records.rename(columns={'Air50P':\"Airtightness\",'FloorArea':\"Floor Area\", 'MainWallIns':\"Main Wall Insulation\" , 'YearBuilt':\"Year Built\", \"Rating\":\"EnerGuide Rating\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-liabilities",
   "metadata": {},
   "source": [
    "Remove missing houses (rows) that have missing data in at least one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-sphere",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Drop the rows where at least one element is NA.\n",
    "cleaned_data = ers_sample_records.dropna()\n",
    "removed = len(ers_sample_records )-len(cleaned_data)\n",
    "#print amount of rows removed \n",
    "print('rows of data dropped:%d' %removed )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-animation",
   "metadata": {},
   "source": [
    "Remove houses (rows) that have inconsistent data. The definition of inconsistent depends on the variable, ensure to change this area when using a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-provincial",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#input bounds for each paramter\n",
    "filt_data = cleaned_data[cleaned_data['Airtightness'] > 0]\n",
    "filt_data = filt_data[filt_data['EnerGuide Rating'] > 0]\n",
    "filt_data = filt_data[filt_data['Year Built'] > 0]\n",
    "filt_data = filt_data[filt_data['Floor Area'] > 0]\n",
    "filt_data = filt_data[filt_data['Main Wall Insulation'] > 0]\n",
    "\n",
    "#print amount of rows removed \n",
    "removed = len(cleaned_data )-len(filt_data)\n",
    "print('rows of data dropped:%d' %removed )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-discretion",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#inintalize scaler \n",
    "scaled = MinMaxScaler()\n",
    "\n",
    "#scale filtered data in preparation for plotting \n",
    "scaled_filt_data = pd.DataFrame(scaled.fit_transform(filt_data))\n",
    "\n",
    "#plot boxplots before outlier removal\n",
    "\n",
    "scaled_filt_data.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-welcome",
   "metadata": {},
   "source": [
    "### Outiler Removal \n",
    "Local outlier factor algorithm detects the outliers and they are subsequently removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-portuguese",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define the model\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20)\n",
    "lof_pred = lof.fit_predict(scaled_filt_data) \n",
    "\n",
    "#extract the negative outputs as the outliers.\n",
    "mask = lof_pred != -1\n",
    "\n",
    "#remove rows with outliers \n",
    "lof_data = filt_data[mask] \n",
    "\n",
    "#print amount of points deleted\n",
    "\n",
    "outliers_rem = len(filt_data) - lof_data.shape[0]\n",
    "print ('Outliers removed: %d' %outliers_rem )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-demonstration",
   "metadata": {},
   "source": [
    "To visualize the outlier removal the parameters are scaled using standardscaler(z-score) so that they can be compared on the same axes then plotted with boxplots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-mexico",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#inintalize scaler \n",
    "scaled = StandardScaler()\n",
    "\n",
    "#scale filtered data in preparation for plotting \n",
    "scaled_filt_data = pd.DataFrame(scaled.fit_transform(filt_data))\n",
    "\n",
    "#plot boxplots before outlier removal\n",
    "\n",
    "scaled_filt_data.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-airfare",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scale data with outliers removed in preparation for plotting\n",
    "\n",
    "scaled_parameters = scaled.fit_transform(lof_data)\n",
    "scaled_parameters = pd.DataFrame(scaled_parameters)\n",
    "\n",
    "#plot boxplots after outlier removal\n",
    "scaled_parameters.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-cheat",
   "metadata": {},
   "source": [
    "## Parameter selection \n",
    "The minimal amount of parameters that represent the building stock data effectively are selected to be clustered. The parameters that are not selected are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the column name of the selected parameters that will be held by the varibale parameters\n",
    "parameters = ['Airtightness', 'Year Built', 'Floor Area',\n",
    "       'Main Wall Insulation']\n",
    "# create a new data frame with parameters \n",
    "cl_data = lof_data[parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "_1950less = lof_data[(lof_data['Year Built'] <= 1950)]\n",
    "_1950_1980 = lof_data[(lof_data['Year Built'] >= 1950) & (lof_data['Year Built'] < 1980)]\n",
    "_1980plus = lof_data[(lof_data['Year Built'] >= 1980)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the column name of the selected parameters that will be held by the varibale parameters\n",
    "#parameters = ['Air50P', 'FloorArea', 'MainWallIns']\n",
    "# create a new data frame with parameters \n",
    "#cl_data = parameters\n",
    "cl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-winning",
   "metadata": {},
   "source": [
    "### Parameter scaling \n",
    "K-means is distance based so for it to consider all attributes as equal and produce unbiased resutls, they must all have the same scale. Minmax scaling rescales the data into a given range (0-1), this method was chosen from earlier 1D/2D clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalize scaler\n",
    "minmax = MinMaxScaler()\n",
    "#scale parameters\n",
    "scaled_parameters = minmax.fit_transform(cl_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-ballet",
   "metadata": {},
   "source": [
    "## K-means clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-bronze",
   "metadata": {},
   "source": [
    "###  Optmial cluster amout determination\n",
    "Silhouette, Davie-boudin, calinski-harabasz, and inertia are calculated for a range of k-values (2-11) and the k-value. The chosen value is then used as an input for the k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_score = []\n",
    "\n",
    "kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",\n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,\n",
    "}\n",
    "\n",
    "for k in range(2,11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(scaled_parameters)\n",
    "    pred_labels = kmeans.labels_\n",
    "    dav_score = davies_bouldin_score(scaled_parameters, pred_labels)\n",
    "    DB_score.append(dav_score)\n",
    "   \n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(2,11), DB_score)\n",
    "plt.xticks(range(2,11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"DB\")\n",
    "plt.show()\n",
    "\n",
    "#find min BD = optimum K\n",
    "\n",
    "#amount of clusters for DB\n",
    "min_index = DB_score.index(min(DB_score))\n",
    "DB_k =range(2,11)\n",
    "DB_k = DB_k[min_index]\n",
    "DB_k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "\n",
    "visualizer = KElbowVisualizer(model, k=(2,10),metric='silhouette')\n",
    "\n",
    "visualizer.fit(scaled_parameters)        # Fit the data to the visualizer\n",
    "visualizer.show()    \n",
    "\n",
    "#plot elbow method with distortion score\n",
    "\n",
    "visualizer = KElbowVisualizer(model, k=(1,10))\n",
    "\n",
    "visualizer.fit(scaled_parameters)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "\n",
    "#plot elbow method Calinski Harabasz \n",
    "\n",
    "visualizer = KElbowVisualizer(model, k=(2,10),metric='calinski_harabasz')\n",
    "\n",
    "visualizer.fit(scaled_parameters)        # Fit the data to the visualizer\n",
    "visualizer.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-credits",
   "metadata": {},
   "source": [
    "### Clustering analysis \n",
    "The prepared data is now clustered using the opitmal amount of clusters found in the past step as the n_clusters argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters=2,#input amount of clusters here\n",
    "    n_init=10,  \n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "\n",
    "kmeans.fit(scaled_parameters)\n",
    "labels = pd.DataFrame(kmeans.labels_,columns=['Cluster Label']) #creating a dataframe to house cluster labels for each house\n",
    "centroids = kmeans.cluster_centers_ #creating varible to hold centroid values (scaled) \n",
    "centroids_real = minmax.inverse_transform(centroids) #centriods with non-scaled values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-estonia",
   "metadata": {},
   "source": [
    "A new dataframe is created adding a column 'label' to show the rows corresponding cluster label decided during k-means clustering. This will allow the resulting partitions to be viewed easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_data_new_index = cl_data.reset_index(drop=True) #new index to allow concatenation to line up properly  \n",
    "#can concatenate along columns but ensure that indices are the same\n",
    "clusters = pd.concat([labels,cl_data_new_index], axis = 1) \n",
    "#make variable for label groups\n",
    "label_group = clusters.groupby(['Cluster Label']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-mexico",
   "metadata": {},
   "source": [
    "A new dataframe is created adding a column 'label' to show the rows corresponding cluster label decided during k-means clustering. This will allow the resulting partitions to be viewed easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_clusters = pd.DataFrame(scaled_parameters, columns = parameters)\n",
    "#can concatenate along columns but ensure that indices are the same\n",
    "scaled_clusters = pd.concat([labels,scaled_clusters], axis = 1) \n",
    "#make variable for label groups\n",
    "scaled_label_group = scaled_clusters.groupby(['Cluster Label']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-process",
   "metadata": {},
   "source": [
    "## Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-talent",
   "metadata": {},
   "source": [
    "### Result Statistics \n",
    "The statistics are a way to evaluate the results numerically. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-liberal",
   "metadata": {},
   "source": [
    "The amount of objects (houses) in each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-snapshot",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_group.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-jonathan",
   "metadata": {},
   "source": [
    "The mean(centroid), standard deviation, and range for each clusters and parameter. Smaller std mean tighter clusters (more compact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-northern",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_group.agg(['std','min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-handling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaled_label_group.agg(['std','min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-armor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_centriods_real = pd.DataFrame(centroids_real, columns = parameters)\n",
    "df_centriods_real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-catalog",
   "metadata": {},
   "source": [
    "The centroids values for each parameter. These represent the archetypes characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-conversion",
   "metadata": {},
   "source": [
    "### Visualize results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-chick",
   "metadata": {},
   "source": [
    "The resulting clusters plotted using parallel coordinates colour coded by cluster label help to comprehend the distribution of each cluster and how distinct the clusters is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-impact",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.parallel_coordinates(clusters, color=\"Cluster Label\", \n",
    "                             color_continuous_scale=px.colors.diverging.Tealrose)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-conviction",
   "metadata": {},
   "source": [
    "The centriods of the resulting clusters plotted using a parallel coordniates graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-people",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#preparing data so that it can be plotted with plotly\n",
    "mem = label_group.agg(['mean'])\n",
    "mem.reset_index(level=0, inplace=True)\n",
    "mem\n",
    "\n",
    "#create a variable to plot centriods in parallel coordinates graph label\n",
    "\n",
    "label = ['Cluster Label']\n",
    "parm = label + parameters\n",
    "centroids_real_pc =  pd.DataFrame(np.array(mem), columns = parm)\n",
    "\n",
    "\n",
    "fig = px.parallel_coordinates(centroids_real_pc, color=\"Cluster Label\" )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-poland",
   "metadata": {},
   "source": [
    "Box plots for each parameter also show how much overlap there is between clusters for each parameter and how compact each cluster is. IF the IQR of each cluster are not offset they are not distinct. The smaller the IQR and whiskers are the more compact the cluster is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-prayer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set up subplot\n",
    "fig, axes = plt.subplots(1, len(parameters), figsize=(18, 10))\n",
    "\n",
    "#use a while loop to plot each parameters distribution\n",
    "i=0\n",
    "while i != len(parameters):\n",
    "    \n",
    "    xx = sns.boxplot(ax=axes[i], x=\"Cluster Label\", y= parameters[i], data=clusters)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-adelaide",
   "metadata": {},
   "source": [
    "Boxplot of normalized results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-swing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#set up subplot\n",
    "fig, axes = plt.subplots(1, len(parameters), figsize=(18, 10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "#use a while loop to plot each parameters distribution\n",
    "i=0\n",
    "while i != len(parameters):\n",
    "    \n",
    "    xx = sns.boxplot(ax=axes[i], x=\"Cluster Label\", y= parameters[i], data=scaled_clusters)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = InterclusterDistance(kmeans)\n",
    "\n",
    "visualizer.fit(scaled_parameters)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "339px",
    "width": "282px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
