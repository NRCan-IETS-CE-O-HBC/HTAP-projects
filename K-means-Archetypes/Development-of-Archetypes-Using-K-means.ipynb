{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "anticipated-mason",
   "metadata": {},
   "source": [
    "# Creating housing archetypes using K-means method\n",
    "### Q4 2020-21\n",
    "\n",
    "## Introduction\n",
    "This workbook will ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from kneed import KneeLocator \n",
    "from sklearn.datasets import make_blobs \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from itertools import permutations\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-definition",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "The data represents a subset of ERS records used for initial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-blond",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ers_sample_records = pd.read_csv(r\"C:\\Users\\owner\\Documents\\NRCan\\code\\practice\\InitialHousingData.csv\",)\n",
    "ers_sample_records \n",
    "\n",
    "#original amount of obsevations/rows \n",
    "og_obs = ers_sample_records.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "ers_sample_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-world",
   "metadata": {},
   "source": [
    "## Database Preperation and Varibable Selection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-drink",
   "metadata": {},
   "source": [
    "### Select variables for clustering\n",
    "Placeholder for this process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Air50P selected for 1D clustering test\n",
    "cl_variables = ['Air50P']\n",
    "test_data = ers_sample_records[cl_variables]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-primary",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-stock",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove rows with blank values\n",
    "test_data_cleaned = test_data.dropna()\n",
    "#display how many rows removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-capital",
   "metadata": {},
   "source": [
    "###  Inconsistent data removal \n",
    "each variable has own range that is should be within. Airttightness >0 ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-chicago",
   "metadata": {},
   "source": [
    "### Ensure all values are numerical\n",
    "and positive? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is catagorical information use LabelEncoder,onehotenoder from scikitlearn..\n",
    "\n",
    "# ensure all vales are floats \n",
    "test_data_cleaned.astype('float') #float64?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-stuart",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-audience",
   "metadata": {},
   "source": [
    "Visualize with box plot to confirm outlier removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-barcelona",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#box plot before outlier removal\n",
    "test_data_cleaned.boxplot() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-murray",
   "metadata": {},
   "source": [
    "#### Local outlier factor\n",
    "Local outlier factor (LOF) values identify an outlier based on the local neighborhood. It gives better results than the global approach to find outliers. A point will be considered as an outlier if it is at a small distance to the extremely dense cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-softball",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define the model\n",
    "\n",
    "lof = LocalOutlierFactor()\n",
    "lof_pred = lof.fit_predict(test_data_cleaned) \n",
    "\n",
    "#extract the negative outputs as the outliers.\n",
    "mask = lof_pred != -1\n",
    "\n",
    "#remove rows with outliers \n",
    "lof_data = test_data_cleaned[mask] #i dont think this works for sets with more than 1 varibale\n",
    "print([lof_data])\n",
    "\n",
    "#print amount of points deleated\n",
    "\n",
    "outliers_rem = og_obs - lof_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )\n",
    "\n",
    "#plot without after removal\n",
    "\n",
    "lof_data.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-mechanism",
   "metadata": {},
   "source": [
    "#### Z-score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find absolute value of z-score for each observation\n",
    "z = np.abs(stats.zscore(test_data_cleaned))\n",
    "\n",
    "#only keep rows in dataframe with all z-scores less than absolute value of 3 \n",
    "z_data = test_data_cleaned[(z<3).all(axis=1)]\n",
    "\n",
    "#print amount of outliers removed\n",
    "z_data\n",
    "\n",
    "outliers_rem = og_obs - z_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotafter outlier removal\n",
    "z_data.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-nicaragua",
   "metadata": {},
   "source": [
    "#### IRQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find Q1, Q3, and interquartile range for each column\n",
    "Q1 =test_data_cleaned.quantile(q=.25)\n",
    "Q3 = test_data_cleaned.quantile(q=.75)\n",
    "IQR =test_data_cleaned.apply(stats.iqr)\n",
    "\n",
    "#only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3\n",
    "iqr_data = test_data_cleaned[~((test_data_cleaned < (Q1-1.5*IQR)) | (test_data_cleaned > (Q3+1.5*IQR))).any(axis=1)]\n",
    "\n",
    "#print amount of outliers removed\n",
    "\n",
    "outliers_rem = og_obs - iqr_data.shape[0]\n",
    "print ('amount of outliers removed: %d' %outliers_rem )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of IQR outlier removal\n",
    "iqr_data.boxplot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-concert",
   "metadata": {},
   "source": [
    " Mahalanobis distance - calc when multi varibale later\n",
    " https://www.statology.org/mahalanobis-distance-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-killer",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "Find the best mix of parameters for clustering. Parameters include oulier removal, scaling, initalization, and k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-basketball",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create sets of pre-processed data ready for clustering \n",
    "scaled_data_sets = [iqr_data,lof_data, z_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of scalers\n",
    "standard = StandardScaler()\n",
    "minimax = MinMaxScaler()\n",
    "scalers = [standard, minimax]\n",
    "#list of initalizers \n",
    "r = 'random'\n",
    "plus = 'k-means++' \n",
    "initalizer = [r, plus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets being transfromed with standard scaler and random init\n",
    "standard_rand=[]\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[0].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elblw method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"random\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    standard_rand.append(kmeans.inertia_) # can use other validations too \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets being transfromed with mini scaler and random init\n",
    "mini_rand=[]\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[1].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elblw method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"random\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    mini_rand.append(kmeans.inertia_) # can use other validations too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets being transfromed with mini scaler and k++ init\n",
    "mini_plus=[]\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[1].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elblw method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    mini_plus.append(kmeans.inertia_) # can use other validations too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets being transfromed with standard scaler and k++ init\n",
    "standard_plus=[]\n",
    "for x in range(len(scaled_data_sets)):\n",
    "    #first set of data to scale\n",
    "    scaled_features = scalers[0].fit_transform(scaled_data_sets[x]) #not going to work for if sets change\n",
    "   \n",
    "    #determine amount of clusters elbow elbow method\n",
    "    \n",
    "    sse=[] #determine SSE for 1 to 11 clusters\n",
    "    \n",
    "    kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\",  #for random \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "    \n",
    "    for k in range(1,11): #pre determined amount\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "    clamount=k1.elbow\n",
    "    \n",
    "    #cluster with amount of clusters \n",
    "    \n",
    "    kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)\n",
    "    \n",
    "    kmeans.fit(scaled_features)\n",
    "    \n",
    "    standard_plus.append(kmeans.inertia_) # can use other validations too \n",
    "\n",
    "    \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-input",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "results = pd.DataFrame({ 'st rand':standard_rand, 'st plus':standard_plus,'min rand':mini_rand, 'min plus': mini_plus}, index= ['IRQ','LOF','Z-score'])\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose best scheme based on interia \n",
    "# lower the interia the better\n",
    "# should use more validaion parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-simple",
   "metadata": {},
   "source": [
    "## Result visualization \n",
    "Visulaizing the results of one method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-packaging",
   "metadata": {},
   "source": [
    "LOF outlier ommision, MinMax scaling, elbow method k determination, k-means++ initalization. Independant od Clustering performed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "data = lof_data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging original data with cleaned data for plotting realtionships later\n",
    "lof_data_full = pd.merge(data, ers_sample_records, right_index=True, left_index =True) \n",
    "lof_data_full\n",
    "lof_data_full = lof_data_full.drop(columns=['Air50P_x'])\n",
    "#re-setting index for mergeing with cluster lables later\n",
    "data_all = lof_data_full\n",
    "data_all = data_all.reset_index(drop=True)\n",
    "data_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling with MinMax scaler\n",
    "minmax = MinMaxScaler()\n",
    "scaled_features = minmax.fit_transform(data)\n",
    "#scaled feature into a data frame \n",
    "scaled_features = pd.DataFrame(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-relative",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#compare Stats for scaled and unsclaed data\n",
    "#show scaled stats\n",
    "scaled_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show unscaled stats\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-recording",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot histogram of scaled feaures \n",
    "scaled_features.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-personal",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering\n",
    "\n",
    "kmeans_kwargs = {\n",
    "    \"init\":\"k-means++\", \n",
    "    \"n_init\":10, \n",
    "    \"max_iter\":300,\n",
    "    \"random_state\":42,}\n",
    "\n",
    "sse=[] #determine SSE for 1 to 11 clusters    \n",
    "for k in range(1,11): \n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "        \n",
    "k1 = KneeLocator(range(1,11),sse,curve=\"convex\", direction=\"decreasing\")\n",
    "clamount=k1.elbow\n",
    "    \n",
    "#cluster with k determined above \n",
    "    \n",
    "kmeans = KMeans(\n",
    "    init=\"k-means++\",\n",
    "    n_clusters= clamount,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-amber",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a dataframe including all variabels and cluster labels \n",
    "kmeans.fit(scaled_features)\n",
    "labels = pd.DataFrame(kmeans.labels_,columns=['cluster label'])\n",
    "\n",
    "cluster1=pd.concat([labels,data_all], axis = 1) \n",
    "\n",
    "cluster1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-kennedy",
   "metadata": {},
   "source": [
    "Looking at clusters individually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-massachusetts",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cluster 0 dataframe\n",
    "cluster_0 = cluster1.loc[cluster1['cluster label'] == 0, ['Air50P_y']]\n",
    "cluster_0.boxplot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-space",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cluster 1 dataframe\n",
    "cluster_1 = cluster1.loc[cluster1['cluster label'] == 1, ['Air50P_y']]\n",
    "cluster_1.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-devon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cluster 2 dataframe\n",
    "cluster_2 = cluster1.loc[cluster1['cluster label'] == 2, ['Air50P_y']]\n",
    "cluster_2.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluser Centroids \n",
    "\n",
    "centroids = minmax.inverse_transform(kmeans.cluster_centers_) # transform scaled cenroids back\n",
    "\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-philosophy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Scatter plot of custers \n",
    "sns.scatterplot('Air50P_y', 'YearBuilt', data=cluster1, hue= 'cluster label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-calvin",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot all varibles against eachother to find patterns in clusters  \n",
    "sns.pairplot(cluster1, hue='cluster label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-negotiation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#silhouette coefficent visualizer\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "\n",
    "visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n",
    "\n",
    "visualizer.fit(scaled_features)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "#In SilhouetteVisualizer plots, clusters with higher scores have wider silhouettes, but clusters \n",
    "#that are less cohesive will fall short of the average score across all clusters, which is plotted as a\n",
    "#vertical dotted red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inter cluster distance maps\n",
    "from yellowbrick.cluster import InterclusterDistance\n",
    "\n",
    "visualizer = InterclusterDistance(kmeans)\n",
    "\n",
    "visualizer.fit(scaled_features)    # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "#the closer to centers are in the visualization, the closer they are in the original feature space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
